{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7Ih0CfNODItq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "7Ih0CfNODItq",
    "outputId": "a326bd9e-67c2-4342-be88-3d37421ea6c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\\n!apt-get update -qq 2>&1 > /dev/null\\n!apt-get -y install -qq google-drive-ocamlfuse fuse\\nfrom google.colab import auth\\nauth.authenticate_user()\\nfrom oauth2client.client import GoogleCredentials\\ncreds = GoogleCredentials.get_application_default()\\nimport getpass\\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\\nvcode = getpass.getpass()\\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08eGFWBxDb6J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "08eGFWBxDb6J",
    "outputId": "266ba2e6-5b82-4aee-ceaa-6ddde835777d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!mkdir -p drive\\n!google-drive-ocamlfuse drive\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7FPYsuGcD9l7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7FPYsuGcD9l7",
    "outputId": "604976d5-dcde-4eae-abe2-6a5ed4c6562e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7wFhHPc5RKif",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wFhHPc5RKif",
    "outputId": "ed1f297d-a69b-435e-bf03-040543c24e91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /Users/dong/opt/anaconda3/lib/python3.8/site-packages (0.8.2)\r\n",
      "Requirement already satisfied: numpy in /Users/dong/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (1.19.2)\r\n",
      "Requirement already satisfied: torch in /Users/dong/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (1.8.1)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/dong/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (8.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/dong/opt/anaconda3/lib/python3.8/site-packages (from torch->torchvision) (3.7.4.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tFsQizA6kUmx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFsQizA6kUmx",
    "outputId": "c7a80716-db32-45a2-ef2c-9204c85c94c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "qPZGxjcmWiOG",
   "metadata": {
    "id": "qPZGxjcmWiOG"
   },
   "outputs": [],
   "source": [
    "#!pip install mxnet-cu112 # 没用着呢\n",
    "#!pip install minpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "smjt0fCkRKzZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smjt0fCkRKzZ",
    "outputId": "7578142e-d274-4903-a4ce-2f25e553c38d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.7423e+30, 4.7393e+30, 9.5461e-01],\n",
      "        [4.4377e+27, 1.7975e+19, 4.6894e+27],\n",
      "        [7.9463e+08, 3.2604e-12, 2.6209e+20],\n",
      "        [4.1641e+12, 1.9434e-19, 3.0881e+29],\n",
      "        [6.3828e+28, 1.4603e-19, 7.7179e+28]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "x = torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6694aee5",
   "metadata": {
    "id": "6694aee5"
   },
   "source": [
    "# main.py 前面import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9914fdc7",
   "metadata": {
    "id": "9914fdc7"
   },
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# Python\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import numpy as np  #注意这里改了，改成能用GPU的numpy了\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from numpy.core._multiarray_umath import ndarray\n",
    "from sklearn.cluster import KMeans\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Torchvison\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import CIFAR100, CIFAR10\n",
    "# from influence import *\n",
    "\n",
    "# Utils\n",
    "# import visdom\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# Custom\n",
    "#import models.resnet as resnet\n",
    "#from config import *\n",
    "#from data.sampler import SubsetSequentialSampler\n",
    "\n",
    "# import copy\n",
    "\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe61243",
   "metadata": {
    "id": "fbe61243"
   },
   "source": [
    "# config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d29a58de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "d29a58de",
    "outputId": "99d3b030-beef-4c16-aa9e-45099418eb02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' CIFAR-10 | ResNet-18 | 93.6%\\nNUM_TRAIN = 50000 # N\\nNUM_VAL   = 50000 - NUM_TRAIN\\nBATCH     = 128 # B\\nSUBSET    = NUM_TRAIN # M\\nADDENDUM  = NUM_TRAIN # K\\n\\nMARGIN = 1.0 # xi\\nWEIGHT = 0.0 # lambda\\n\\nTRIALS = 1\\nCYCLES = 1\\n\\nEPOCH = 50\\nLR = 0.1\\nMILESTONES = [25, 35]\\nEPOCHL = 40\\n\\nMOMENTUM = 0.9\\nWDECAY = 5e-4\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "# Learning Loss for Active Learning\n",
    "NUM_TRAIN = 50000 # N\n",
    "NUM_VAL   = 50000 - NUM_TRAIN\n",
    "BATCH     = 128  # 128 # B  注意，这里本来是是128\n",
    "SUBSET    = 25000 # M  # 本来是25000\n",
    "ADDENDUM  = 2500  # K   # 本来是2500\n",
    "\n",
    "MARGIN = 1.0  # xi\n",
    "WEIGHT = 1.0  # 1.0 # lambda\n",
    "\n",
    "TRIALS = 1\n",
    "CYCLES = 7   # 本来应该是7\n",
    "\n",
    "EPOCH = 200   # 本来是200\n",
    "LR = 0.1    # 0.1 for SGD\n",
    "MILESTONES = [160]\n",
    "EPOCHL = 120 # After 120 epochs, stop the gradient from the loss prediction module propagated to the target model\n",
    "\n",
    "MOMENTUM = 0.9\n",
    "WDECAY = 5e-4\n",
    "\n",
    "CLUSTER_NUMS = 10 # 新加的，聚类的cluster 数量\n",
    "CLUSTER_MAX_ITER = 10000 # 新加的，聚类的迭代次数，本来是10000\n",
    "\n",
    "\n",
    "''' CIFAR-10 | ResNet-18 | 93.6%\n",
    "NUM_TRAIN = 50000 # N\n",
    "NUM_VAL   = 50000 - NUM_TRAIN\n",
    "BATCH     = 128 # B\n",
    "SUBSET    = NUM_TRAIN # M\n",
    "ADDENDUM  = NUM_TRAIN # K\n",
    "\n",
    "MARGIN = 1.0 # xi\n",
    "WEIGHT = 0.0 # lambda\n",
    "\n",
    "TRIALS = 1\n",
    "CYCLES = 1\n",
    "\n",
    "EPOCH = 50\n",
    "LR = 0.1\n",
    "MILESTONES = [25, 35]\n",
    "EPOCHL = 40\n",
    "\n",
    "MOMENTUM = 0.9\n",
    "WDECAY = 5e-4\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12149227",
   "metadata": {
    "id": "12149227"
   },
   "source": [
    "# device设置成CPU或GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed3207d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed3207d4",
    "outputId": "6d6fb9a1-6c59-4f13-ab70-c8d69db2578c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# 加一个device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad37c670",
   "metadata": {
    "id": "ad37c670"
   },
   "source": [
    "# sampler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fd8a205",
   "metadata": {
    "id": "8fd8a205"
   },
   "outputs": [],
   "source": [
    "class SubsetSequentialSampler(torch.utils.data.Sampler):\n",
    "    r\"\"\"Samples elements sequentially from a given list of indices, without replacement.\n",
    "\n",
    "    Arguments:\n",
    "        indices (sequence): a sequence of indices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in range(len(self.indices)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c18965",
   "metadata": {
    "id": "08c18965"
   },
   "source": [
    "\n",
    "# main.py 里面的data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdbe408f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdbe408f",
    "outputId": "7f73ad44-54a3-4197-e468-573bd70be541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# Data\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(size=32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "cifar10_train = CIFAR10('../cifar10', train=True, download=True, transform=train_transform)  # specify data path here\n",
    "cifar10_unlabeled = CIFAR10('../cifar10', train=True, download=True, transform=test_transform)\n",
    "cifar10_test = CIFAR10('../cifar10', train=False, download=True, transform=test_transform)\n",
    "\n",
    "##\n",
    "# Train Utils\n",
    "iters = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91114054",
   "metadata": {
    "id": "91114054"
   },
   "source": [
    "# resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36b3fb7f",
   "metadata": {
    "id": "36b3fb7f"
   },
   "outputs": [],
   "source": [
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "        self.linear1 = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out1 = self.layer1(out)\n",
    "        out2 = self.layer2(out1)\n",
    "        out3 = self.layer3(out2)\n",
    "        out4 = self.layer4(out3)\n",
    "        out5 = F.avg_pool2d(out4, 4)\n",
    "        out5 = out5.view(out5.size(0), -1)     # [128, 512]\n",
    "        out = self.linear(out5)\n",
    "        out_cons = self.linear1(out5)\n",
    "\n",
    "        return out, out_cons, out5, [out1, out2, out3, out4]\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet18_student():\n",
    "    return ResNet(BasicBlock, [1,1,1,1])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a2c17",
   "metadata": {
    "id": "280a2c17"
   },
   "source": [
    "# main.py 里面的train_epoch函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e52d13f",
   "metadata": {
    "id": "5e52d13f"
   },
   "outputs": [],
   "source": [
    "#\n",
    "def train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss):\n",
    "    models['backbone'].train()\n",
    "    global iters\n",
    "\n",
    "    for data in dataloaders['train']:\n",
    "        inputs = data[0].to(device)\n",
    "        labels = data[1].to(device)\n",
    "        iters += 1\n",
    "\n",
    "        optimizers['backbone'].zero_grad()\n",
    "\n",
    "        scores, _, _, features_list = models['backbone'](inputs)\n",
    "        target_loss = criterion(scores, labels)\n",
    "\n",
    "        m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)\n",
    "        loss = m_backbone_loss\n",
    "        loss.backward()\n",
    "        optimizers['backbone'].step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8427241c",
   "metadata": {
    "id": "8427241c"
   },
   "source": [
    "# main.py 里面的test函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0a4bf72",
   "metadata": {
    "id": "e0a4bf72"
   },
   "outputs": [],
   "source": [
    "#\n",
    "def test(models, dataloaders, mode='val'):\n",
    "    assert mode == 'val' or mode == 'test'\n",
    "    models['backbone'].eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in dataloaders[mode]:\n",
    "            #inputs = inputs.cuda()这才是prcharm原来的\n",
    "            inputs = inputs.to(device)\n",
    "            #labels = labels.cuda()这才是prcharm原来的\n",
    "            labels = labels.to(device)\n",
    "            scores, _, _, _ = models['backbone'](inputs)\n",
    "            _, preds = torch.max(scores.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c901cd5a",
   "metadata": {
    "id": "c901cd5a"
   },
   "source": [
    "# main.py里面的train函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1e24b49",
   "metadata": {
    "id": "e1e24b49"
   },
   "outputs": [],
   "source": [
    "#\n",
    "def train(models, criterion, optimizers, schedulers, dataloaders, num_epochs, epoch_loss, cycle):\n",
    "    print('>> Train a Model...')\n",
    "    best_acc = 0.\n",
    "    checkpoint_dir = os.path.join('./cifar10', 'train', 'weights')\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss)\n",
    "        schedulers['backbone'].step()\n",
    "\n",
    "        # Save a checkpoint\n",
    "        if epoch % 20 == 0 or epoch == EPOCH - 1:\n",
    "            acc = test(models, dataloaders, 'test')\n",
    "            if best_acc < acc:\n",
    "                best_acc = acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict_backbone': models['backbone'].state_dict()\n",
    "                    # 'state_dict_module': models['module'].state_dict()\n",
    "                },\n",
    "                    '%s/active_resnet18_cifar10.pth' % (checkpoint_dir))\n",
    "            print('Cycle:', cycle + 1, 'Epoch:', epoch + 1, \"---\", 'Val Acc: {:.3f} \\t Best Acc: {:.3f}'.format(acc, best_acc),flush=True)\n",
    "            \n",
    "    print('>> Finished.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b264b6",
   "metadata": {
    "id": "71b264b6"
   },
   "source": [
    "# main.py里面的get_uncertainty函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcc4b5fe",
   "metadata": {
    "id": "bcc4b5fe"
   },
   "outputs": [],
   "source": [
    "#\n",
    "def get_uncertainty(models, unlabeled_loader):\n",
    "    models['backbone'].eval()\n",
    "    #uncertainty = torch.tensor([]).to(device)  #这是原来版本，聚类先不用GPU，用CPU可以用numpy\n",
    "    uncertainty = torch.tensor([])\n",
    "    all_features = torch.tensor([]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print(\">> Start clustering:\")\n",
    "        for (inputs, labels) in unlabeled_loader:\n",
    "            #inputs = inputs.cuda()这才是pycharm原来的\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            scores, _, total_feature, features = models['backbone'](inputs)\n",
    "            # total_feature: [SUBSET,512], SUBSET是行 = 128，是batch_size，也就是图片个数，512是列，是resnet展平的feature个数\n",
    "            all_features = torch.cat((all_features, total_feature), axis = 0)\n",
    "            \n",
    "        # 这是原来版本，计算关键点 因为KMeans.fix(X[,y) X是需要2D 而不是1D\n",
    "        input_x = all_features.cpu().numpy()\n",
    "        line_num, column_num = input_x.shape  # line_num：样本数量，column_num：每个样本的feature里个数\n",
    "\n",
    "        result = np.empty(line_num, dtype=np.int)  # line_num个样本的聚类结果\n",
    "        distance = np.empty((line_num, CLUSTER_NUMS), dtype=np.float32)  \n",
    "\n",
    "        # 从line_num个数据样本中不重复地随机选择k个样本作为质心,cores\n",
    "        cores = input_x[np.random.choice(np.arange(line_num), CLUSTER_NUMS, replace=False)]\n",
    "        min_distance = np.empty((line_num, 1), dtype=np.float32) #用于第二种方法，最远距离不确定度大\n",
    "\n",
    "        iter = 0 #迭代次数\n",
    "        while True:  # 迭代聚类计算,max iteration是10000次，质心都不变时停止\n",
    "            iter = iter + 1\n",
    "            d = np.square(np.repeat(input_x, CLUSTER_NUMS, axis=0).reshape(line_num, CLUSTER_NUMS, column_num) - cores)\n",
    "            distance = np.sqrt(np.sum(d, axis=2))  # ndarray(line_num, k)，每个样本距离 k（CLUSTER_NUM）个质心的距离，共有line_num行\n",
    "\n",
    "            index_min = np.argmin(distance, axis=1)  # 每个样本距离最近的质心索引序号 [line_num, 1]\n",
    "            if (index_min == result).all() or iter == CLUSTER_MAX_ITER:  #质心不变时停止\n",
    "                break;\n",
    "\n",
    "            result[:] = index_min  # 重新分类 [line_num, 1] 这些图片分别属于哪个cluster, index\n",
    "            for i in range(CLUSTER_NUMS):  # 遍历质心集\n",
    "                if len(input_x[result == i]) != 0:\n",
    "                    items = input_x[result == i]       # 找出对应当前质心的子样本集\n",
    "                    cores[i] = np.mean(items, axis=0)  # 以子样本集的均值作为当前质心的位置\n",
    "\n",
    "        sorted_distance = np.sort(distance) # 将矩阵的每一行升序排列\n",
    "        min_distance = np.abs(sorted_distance[:, 0].reshape(line_num, 1)) #min_distance表示该样本距离自己属于的cluster质心的距离\n",
    "        min_distance_torch = torch.from_numpy(min_distance)\n",
    "\n",
    "        #uncertainty = torch.cat((uncertainty, min_distance_torch*10), 0)  #min_distance大小与uncertainty大小正相关\n",
    "            \n",
    "    print(\">> Clustering Over:\")\n",
    "    return min_distance_torch.cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a4edc",
   "metadata": {
    "id": "411a4edc"
   },
   "source": [
    "# main.py 里面的main函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7c84637",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "e7c84637",
    "outputId": "3959a53b-e906-453a-bafe-822092cc5580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Train a Model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-821479b03dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Training and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschedulers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             print('Trial {}/{} || Cycle {}/{} || Label set size {}: Test acc {}'.format(trial + 1, TRIALS, cycle + 1,\n",
      "\u001b[0;32m<ipython-input-16-59a0ecc1ab63>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(models, criterion, optimizers, schedulers, dataloaders, num_epochs, epoch_loss, cycle)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mschedulers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'backbone'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a0e03dbc905d>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(models, criterion, optimizers, dataloaders, epoch, epoch_loss)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'backbone'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'backbone'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtarget_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f7f683d520bf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mout3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mout4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mout5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mout5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# [128, 512]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f7f683d520bf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "##\n",
    "# Main\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for trial in range(TRIALS):\n",
    "        # Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
    "        indices = list(range(NUM_TRAIN))\n",
    "        random.shuffle(indices)\n",
    "        START = 2 * ADDENDUM\n",
    "        labeled_set = indices[:START]\n",
    "        unlabeled_set = indices[START:]\n",
    "\n",
    "        train_loader = DataLoader(cifar10_train, batch_size=BATCH,  # BATCH\n",
    "                                  sampler=SubsetRandomSampler(labeled_set),\n",
    "                                  pin_memory=True)\n",
    "        test_loader = DataLoader(cifar10_test, batch_size=BATCH)\n",
    "\n",
    "        dataloaders = {'train': train_loader, 'test': test_loader}\n",
    "\n",
    "        # Model\n",
    "        #backbone_net = resnet.ResNet18().cuda() #记住这才是原本pycharm里面的\n",
    "        backbone_net = ResNet18().to(device)\n",
    "        \n",
    "        models = {'backbone': backbone_net}\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        # Active learning cycles\n",
    "        for cycle in range(CYCLES):\n",
    "            # Loss, criterion and scheduler (re)initialization\n",
    "            criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "            optim_backbone = optim.SGD(models['backbone'].parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WDECAY)\n",
    "\n",
    "            sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=MILESTONES)\n",
    "\n",
    "            optimizers = {'backbone': optim_backbone}\n",
    "            schedulers = {'backbone': sched_backbone}\n",
    "\n",
    "            # Training and test\n",
    "            train(models, criterion, optimizers, schedulers, dataloaders, EPOCH, EPOCHL, cycle)\n",
    "            acc = test(models, dataloaders, mode='test')\n",
    "            print('Trial {}/{} || Cycle {}/{} || Label set size {}: Test acc {}'.format(trial + 1, TRIALS, cycle + 1,\n",
    "                                                                                        CYCLES, len(labeled_set), acc),\n",
    "                                                                                         flush=True)\n",
    "\n",
    "            ##\n",
    "            #  Update the labeled dataset via loss prediction-based uncertainty measurement\n",
    "\n",
    "            # Randomly sample 10000 unlabeled data points\n",
    "            random.shuffle(unlabeled_set)\n",
    "            subset = unlabeled_set[:SUBSET]\n",
    "\n",
    "            # Create unlabeled dataloader for the unlabeled subset\n",
    "            unlabeled_loader = DataLoader(cifar10_unlabeled, batch_size=BATCH,\n",
    "                                          sampler=SubsetSequentialSampler(subset),\n",
    "                                          # more convenient if we maintain the order of subset\n",
    "                                          pin_memory=True)\n",
    "\n",
    "            # Measure uncertainty of each data points in the subset\n",
    "            uncertainty = get_uncertainty(models, unlabeled_loader)\n",
    "            #print(\"main uncertainty: \", uncertainty)\n",
    "            uncertainty = uncertainty.T\n",
    "            # Index in ascending order\n",
    "            arg = np.argsort(uncertainty).numpy().tolist()\n",
    "            #print(\"main arg: \", arg) \n",
    "            \n",
    "            # Update the labeled dataset and the unlabeled dataset, respectively\n",
    "            labeled_set += list(torch.tensor(subset)[arg][-ADDENDUM:].numpy())  # select largest loss\n",
    "            unlabeled_set = list(torch.tensor(subset)[arg][:-ADDENDUM].numpy()) + unlabeled_set[SUBSET:]\n",
    "\n",
    "            # labeled_set += list(torch.tensor(subset)[arg][:ADDENDUM].numpy())  # select smallest influence\n",
    "            # unlabeled_set = list(torch.tensor(subset)[arg][ADDENDUM:].numpy()) + unlabeled_set[SUBSET:]\n",
    "\n",
    "            # Create a new dataloader for the updated labeled dataset\n",
    "            dataloaders['train'] = DataLoader(cifar10_train, batch_size=BATCH,  # BATCH\n",
    "                                              sampler=SubsetRandomSampler(labeled_set),\n",
    "                                              pin_memory=True)\n",
    "\n",
    "        # Save a checkpoint\n",
    "        torch.save({\n",
    "            'trial': trial + 1,\n",
    "            'state_dict_backbone': models['backbone'].state_dict()\n",
    "            # 'state_dict_module': models['module'].state_dict()\n",
    "        },\n",
    "            './cifar10/train/weights/active_resnet18_cifar10_trial{}.pth'.format(trial))\n",
    "\n",
    "        print('---------------------------Current Trial is done-----------------------------',flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21adede",
   "metadata": {
    "id": "e21adede"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "c10_AL_code_第二版_距离.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
