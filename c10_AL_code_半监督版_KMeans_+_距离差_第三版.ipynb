{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "c10_AL_code_半监督版_KMeans_+_距离差_第二版.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6668a6d7ce814a83a39924a84664c410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_237abf371a434f3f8af7973397587a08",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_af5e7cc4e3d148d292db1a09e1d605a4",
              "IPY_MODEL_4ad88735cba844e189b8da304789d481"
            ]
          }
        },
        "237abf371a434f3f8af7973397587a08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af5e7cc4e3d148d292db1a09e1d605a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1470fd03c56f4810ba3d9f71616ae1c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d9db7b88d5c4011b09bc87e4fb30d3e"
          }
        },
        "4ad88735cba844e189b8da304789d481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_51f3caead7874313a9ba8fd26f100ee4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [01:50&lt;00:00, 1548619.36it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a1a65dda2fcd4f9bbfecabcbb6636dd0"
          }
        },
        "1470fd03c56f4810ba3d9f71616ae1c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d9db7b88d5c4011b09bc87e4fb30d3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "51f3caead7874313a9ba8fd26f100ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a1a65dda2fcd4f9bbfecabcbb6636dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "7Ih0CfNODItq",
        "outputId": "132b6647-072b-47a6-bb81-93e6478c1d0f"
      },
      "source": [
        "'''\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "'''\n"
      ],
      "id": "7Ih0CfNODItq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\\n!apt-get update -qq 2>&1 > /dev/null\\n!apt-get -y install -qq google-drive-ocamlfuse fuse\\nfrom google.colab import auth\\nauth.authenticate_user()\\nfrom oauth2client.client import GoogleCredentials\\ncreds = GoogleCredentials.get_application_default()\\nimport getpass\\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\\nvcode = getpass.getpass()\\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "08eGFWBxDb6J",
        "outputId": "d7d39ac3-2bda-47b5-bccd-56416cf413e2"
      },
      "source": [
        "'''\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "'''"
      ],
      "id": "08eGFWBxDb6J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!mkdir -p drive\\n!google-drive-ocamlfuse drive\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7FPYsuGcD9l7",
        "outputId": "2512e8ad-6402-4489-e243-5949071b8c59"
      },
      "source": [
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "'''"
      ],
      "id": "7FPYsuGcD9l7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7wFhHPc5RKif",
        "outputId": "088cee8e-ca4f-4e23-f746-b9b2cac1c793"
      },
      "source": [
        "'''\n",
        "!pip install torchvision\n",
        "'''"
      ],
      "id": "7wFhHPc5RKif",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!pip install torchvision\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFsQizA6kUmx",
        "outputId": "6e6ad4b8-55bd-4e14-dc5a-38d98154badb"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "tFsQizA6kUmx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 28 04:53:07 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPZGxjcmWiOG"
      },
      "source": [
        "#!pip install mxnet-cu112 # 没用着呢\n",
        "#!pip install minpy"
      ],
      "id": "qPZGxjcmWiOG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smjt0fCkRKzZ",
        "outputId": "ee21b910-27bb-4c4a-922b-a163ceb288f1"
      },
      "source": [
        "import torch \n",
        "x = torch.empty(5,3)\n",
        "print(x)"
      ],
      "id": "smjt0fCkRKzZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-2.8933e-22,  3.0824e-41,  3.3631e-44],\n",
            "        [ 0.0000e+00,         nan,  3.0824e-41],\n",
            "        [ 1.1578e+27,  1.1362e+30,  7.1547e+22],\n",
            "        [ 4.5828e+30,  1.2121e+04,  7.1846e+22],\n",
            "        [ 9.2198e-39,  7.0374e+22, -7.5359e-23]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6694aee5"
      },
      "source": [
        "# main.py 前面import"
      ],
      "id": "6694aee5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9914fdc7"
      },
      "source": [
        "# -*- coding: UTF-8 -*-\n",
        "# Python\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np  #注意这里可以改用Gpu的Numpy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from numpy.core._multiarray_umath import ndarray\n",
        "from sklearn.cluster import KMeans\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# image \n",
        "from PIL import Image\n",
        "\n",
        "# Torchvison\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import CIFAR100, CIFAR10\n",
        "# from influence import *\n",
        "\n",
        "# Utils\n",
        "# import visdom\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# Custom\n",
        "#import models.resnet as resnet\n",
        "#from config import *\n",
        "#from data.sampler import SubsetSequentialSampler\n",
        "\n",
        "# import copy\n",
        "\n",
        "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
      ],
      "id": "9914fdc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbe61243"
      },
      "source": [
        "# config.py"
      ],
      "id": "fbe61243"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "d29a58de",
        "outputId": "818a5cb3-01c9-48be-e4fc-b7b14287e408"
      },
      "source": [
        "##\n",
        "# Learning Loss for Active Learning\n",
        "\n",
        "# Config\n",
        "NUM_TRAIN = 50000 # N 训练集数据个数\n",
        "NUM_VAL   = 50000 - NUM_TRAIN\n",
        "BATCH     = 128  # 128 # 每个dataloader的图片个数\n",
        "SUBSET    = 25000 # M  # 每次在未标签的数据集中选来聚类的图片个数\n",
        "ADDENDUM  = 2500  # K  # 每个cycle新加入的不确定度最大的图片个数\n",
        "\n",
        "MARGIN = 1.0  # xi\n",
        "WEIGHT = 1.0  # 1.0 # lambda\n",
        "\n",
        "TRIALS = 1 #尝试 一次\n",
        "CYCLES = 7  #迭代数\n",
        "\n",
        "EPOCH = 200  #神经网络训练轮数\n",
        "LR = 0.1    # 0.1 for SGD # 初始学习速率\n",
        "MILESTONES = [160] \n",
        "EPOCHL = 120 # \n",
        "\n",
        "MOMENTUM = 0.9 # optimizer优化器参数之一\n",
        "WDECAY = 5e-4 # optimizer优化器参数之一\n",
        "\n",
        "CLUSTER_NUMS = 10 # Kmeans 选取的K值\n",
        "CLUSTER_MAX_ITER = 10000#聚类最大迭代次数\n",
        "\n",
        "SEMI_WEIGHT = 0.001\n",
        "\n",
        "''' CIFAR-10 | ResNet-18 | 93.6%\n",
        "NUM_TRAIN = 50000 # N\n",
        "NUM_VAL   = 50000 - NUM_TRAIN\n",
        "BATCH     = 128 # B\n",
        "SUBSET    = NUM_TRAIN # M\n",
        "ADDENDUM  = NUM_TRAIN # K\n",
        "\n",
        "MARGIN = 1.0 # xi\n",
        "WEIGHT = 0.0 # lambda\n",
        "\n",
        "TRIALS = 1\n",
        "CYCLES = 1\n",
        "\n",
        "EPOCH = 50\n",
        "LR = 0.1\n",
        "MILESTONES = [25, 35]\n",
        "EPOCHL = 40\n",
        "\n",
        "MOMENTUM = 0.9\n",
        "WDECAY = 5e-4\n",
        "'''\n"
      ],
      "id": "d29a58de",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' CIFAR-10 | ResNet-18 | 93.6%\\nNUM_TRAIN = 50000 # N\\nNUM_VAL   = 50000 - NUM_TRAIN\\nBATCH     = 128 # B\\nSUBSET    = NUM_TRAIN # M\\nADDENDUM  = NUM_TRAIN # K\\n\\nMARGIN = 1.0 # xi\\nWEIGHT = 0.0 # lambda\\n\\nTRIALS = 1\\nCYCLES = 1\\n\\nEPOCH = 50\\nLR = 0.1\\nMILESTONES = [25, 35]\\nEPOCHL = 40\\n\\nMOMENTUM = 0.9\\nWDECAY = 5e-4\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12149227"
      },
      "source": [
        "# device设置成cuda或GPU "
      ],
      "id": "12149227"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed3207d4",
        "outputId": "43c8c625-d852-4732-fb5f-e653af69dbd0"
      },
      "source": [
        "#\n",
        "# 加一个device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.is_available())"
      ],
      "id": "ed3207d4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad37c670"
      },
      "source": [
        "# sampler.py"
      ],
      "id": "ad37c670"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fd8a205"
      },
      "source": [
        "class SubsetSequentialSampler(torch.utils.data.Sampler):\n",
        "    r\"\"\"Samples elements sequentially from a given list of indices, without replacement.\n",
        "\n",
        "    Arguments:\n",
        "        indices (sequence): a sequence of indices\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, indices):\n",
        "        self.indices = indices\n",
        "\n",
        "    def __iter__(self):\n",
        "        return (self.indices[i] for i in range(len(self.indices)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n"
      ],
      "id": "8fd8a205",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08c18965"
      },
      "source": [
        "\n",
        "# main.py 里面的data"
      ],
      "id": "08c18965"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "6668a6d7ce814a83a39924a84664c410",
            "237abf371a434f3f8af7973397587a08",
            "af5e7cc4e3d148d292db1a09e1d605a4",
            "4ad88735cba844e189b8da304789d481",
            "1470fd03c56f4810ba3d9f71616ae1c0",
            "4d9db7b88d5c4011b09bc87e4fb30d3e",
            "51f3caead7874313a9ba8fd26f100ee4",
            "a1a65dda2fcd4f9bbfecabcbb6636dd0"
          ]
        },
        "id": "cdbe408f",
        "outputId": "7889885e-264e-476f-9eaf-dbb4f823c2fe"
      },
      "source": [
        "##\n",
        "# Data\n",
        "train_transform = T.Compose([\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomCrop(size=32, padding=4),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "cifar10_train = CIFAR10('../cifar10', train=True, download=True, transform=train_transform)  # specify data path here\n",
        "cifar10_unlabeled = CIFAR10('../cifar10', train=True, download=True, transform=test_transform)\n",
        "cifar10_test = CIFAR10('../cifar10', train=False, download=True, transform=test_transform)\n",
        "\n",
        "##\n",
        "# Train Utils\n",
        "iters = 0\n"
      ],
      "id": "cdbe408f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../cifar10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6668a6d7ce814a83a39924a84664c410",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ../cifar10/cifar-10-python.tar.gz to ../cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91114054"
      },
      "source": [
        "# resnet.py"
      ],
      "id": "91114054"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36b3fb7f"
      },
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "        self.linear1 = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out1 = self.layer1(out)\n",
        "        out2 = self.layer2(out1)\n",
        "        out3 = self.layer3(out2)\n",
        "        out4 = self.layer4(out3)\n",
        "        out5 = F.avg_pool2d(out4, 4)\n",
        "        out5 = out5.view(out5.size(0), -1)     # [128, 512]\n",
        "        out = self.linear(out5)\n",
        "        out_cons = self.linear1(out5)\n",
        "\n",
        "        return out, out_cons, out5, [out1, out2, out3, out4]\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet18_student():\n",
        "    return ResNet(BasicBlock, [1,1,1,1])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3,8,36,3])\n",
        "\n"
      ],
      "id": "36b3fb7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "280a2c17"
      },
      "source": [
        "# main.py 里面的train_epoch函数"
      ],
      "id": "280a2c17"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e52d13f"
      },
      "source": [
        "def train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss):\n",
        "    models['backbone'].train()\n",
        "    global iters\n",
        "\n",
        "    for data in dataloaders['train']:\n",
        "        inputs = data[0].to(device)\n",
        "        labels = data[1].to(device)\n",
        "        iters += 1\n",
        "        # print(\"train_inputs: \",inputs.shape,\"extra_labels: \",labels.shape)\n",
        "\n",
        "        optimizers['backbone'].zero_grad()\n",
        "\n",
        "        scores, _, _, features_list = models['backbone'](inputs)\n",
        "        target_loss = criterion(scores, labels)\n",
        "        extra_inputs, extra_labels = next(iter(dataloaders['extra']))  # 新加的 这3行 算聚类后伪标签的loss\n",
        "        # print(\"extra_inputs_shape: \",extra_inputs.shape,\"extra_labels: \",extra_labels)\n",
        "        extra_inputs = extra_inputs.to(device)\n",
        "        # 这儿得在外面给extra_dataloader 用Kmeans聚类一下，得到伪标签,加一列标签\n",
        "        extra_labels = extra_labels.to(device=device, dtype=torch.int64)\n",
        "        extra_scores, _, _, _ = models['backbone'](extra_inputs)\n",
        "        extra_loss = criterion(extra_scores, extra_labels)\n",
        "\n",
        "        backbone_loss = torch.sum(target_loss) / target_loss.size(0)\n",
        "        aver_extra_loss = torch.sum(extra_loss) / extra_loss.size(0)\n",
        "        loss = backbone_loss + SEMI_WEIGHT * aver_extra_loss  # 这里的系数也可以改成 0.1试试\n",
        "\n",
        "        loss.backward()\n",
        "        optimizers['backbone'].step()\n"
      ],
      "id": "5e52d13f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8427241c"
      },
      "source": [
        "# main.py 里面的test函数"
      ],
      "id": "8427241c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0a4bf72"
      },
      "source": [
        "#\n",
        "def test(models, dataloaders, mode='val'):\n",
        "    assert mode == 'val' or mode == 'test'\n",
        "    models['backbone'].eval()\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for (inputs, labels) in dataloaders[mode]:\n",
        "            #inputs = inputs.cuda()这才是prcharm原来的\n",
        "            inputs = inputs.to(device)\n",
        "            #labels = labels.cuda()这才是prcharm原来的\n",
        "            labels = labels.to(device)\n",
        "            scores, _, _, _ = models['backbone'](inputs)\n",
        "            _, preds = torch.max(scores.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "    return 100 * correct / total"
      ],
      "id": "e0a4bf72",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c901cd5a"
      },
      "source": [
        "# main.py里面的train函数"
      ],
      "id": "c901cd5a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1e24b49"
      },
      "source": [
        "#\n",
        "def train(models, criterion, optimizers, schedulers, dataloaders, num_epochs, epoch_loss, cycle):\n",
        "    print('>> Train a Model...')\n",
        "    best_acc = 0.\n",
        "    checkpoint_dir = os.path.join('./cifar10', 'train', 'weights')\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss)\n",
        "        schedulers['backbone'].step()\n",
        "\n",
        "        # Save a checkpoint\n",
        "        if epoch % 20 == 0 or epoch == EPOCH - 1:\n",
        "            acc = test(models, dataloaders, 'test')\n",
        "            if best_acc < acc:\n",
        "                best_acc = acc\n",
        "                torch.save({\n",
        "                    'epoch': epoch + 1,\n",
        "                    'state_dict_backbone': models['backbone'].state_dict()\n",
        "                    # 'state_dict_module': models['module'].state_dict()\n",
        "                },\n",
        "                    '%s/active_resnet18_cifar10.pth' % (checkpoint_dir))\n",
        "            print('Cycle:', cycle + 1, 'Epoch:', epoch + 1, \"---\", 'Val Acc: {:.3f} \\t Best Acc: {:.3f}'.format(acc, best_acc),flush=True)\n",
        "            \n",
        "    print('>> Finished.')\n"
      ],
      "id": "e1e24b49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj7EDCF2fdj_"
      },
      "source": [
        "# main.py里面的get_uncertainty函数"
      ],
      "id": "kj7EDCF2fdj_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szSjZy65fRPS"
      },
      "source": [
        "#\n",
        "def get_uncertainty(models, unlabeled_loader):\n",
        "    models['backbone'].eval()\n",
        "    #uncertainty = torch.tensor([]).to(device) #这是原来版本，聚类先不用GPU，用CPU可以用numpy\n",
        "    uncertainty = torch.tensor([])\n",
        "    all_features = torch.tensor([]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        print(\">> Start clustering:\")\n",
        "\n",
        "        for (inputs, labels) in unlabeled_loader:\n",
        "            # inputs = inputs.cuda()这才是pycharm原来的\n",
        "            inputs = inputs.to(device)\n",
        "            scores, _, total_feature, features = models['backbone'](inputs)\n",
        "            # total_feature: [batch_size,512], 行，batch_size，也就是每个batch图片个数，\n",
        "            # 512是列，是resnet展平的像素feature维度数\n",
        "            all_features = torch.cat((all_features, total_feature), axis=0)\n",
        "            # 从未标签数据集中选出的25000个图片的feature\n",
        "\n",
        "        input_x = np.array(all_features.cpu())\n",
        "        # input_x: shape: [BATCH_SIZE,512]\n",
        "\n",
        "        line_num, column_num = input_x.shape  # line_num：样本数量，column_num：每个样本的属性值个数\n",
        "\n",
        "        cluster_index = np.empty(line_num, dtype=int)  # line_num个样本的聚类结果\n",
        "        distance = np.empty((line_num, CLUSTER_NUMS), dtype=np.float32)\n",
        "\n",
        "        # 从line_num个数据样本中不重复地随机选择k个样本作为质心\n",
        "        cores = input_x[np.random.choice(np.arange(line_num), CLUSTER_NUMS, replace=False)]\n",
        "        iter = 0\n",
        "\n",
        "        while True:  # 迭代聚类计算,max=10000次，质心不变时break\n",
        "            iter = iter + 1\n",
        "            d = np.square(np.repeat(input_x, CLUSTER_NUMS, axis=0).reshape(line_num, CLUSTER_NUMS, column_num)\n",
        "                          - cores)\n",
        "            distance = np.sqrt(np.sum(d, axis=2))  # ndarray(line_num, k)，每个样本距离k个质心的距离，共有line_num行\n",
        "\n",
        "            index_min = np.argmin(distance, axis=1)  # 每个样本距离最近的质心索引序号 [line_num, 1]\n",
        "            if (index_min == cluster_index).all() or iter == CLUSTER_MAX_ITER:\n",
        "                break;\n",
        "\n",
        "            cluster_index[:] = index_min  # 重新分类 [line_num, 1] 这些图片分别属于哪个cluster, index\n",
        "            for i in range(CLUSTER_NUMS):  # 遍历质心集\n",
        "                if len(input_x[cluster_index == i]) != 0:\n",
        "                    items = input_x[cluster_index == i]  # 找出对应当前质心的子样本集\n",
        "                    cores[i] = np.mean(items, axis=0)  # 以子样本集的均值作为当前质心的位置\n",
        "\n",
        "        sorted_distance = np.sort(distance)  # 将矩阵的每一行升序排列\n",
        "\n",
        "        min_distance_differ = np.abs(sorted_distance[:, 0].reshape(line_num, 1)\n",
        "                                     - sorted_distance[:, 1].reshape(line_num, 1))\n",
        "\n",
        "        min_distance_differ_torch = torch.from_numpy(min_distance_differ)\n",
        "\n",
        "        uncertainty = 10.00 / min_distance_differ_torch\n",
        "\n",
        "    print(\">> Clustering Over:\")\n",
        "    return uncertainty.cpu()"
      ],
      "id": "szSjZy65fRPS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_kSS5zKwJ-I"
      },
      "source": [
        "# 创建sudoDataset"
      ],
      "id": "s_kSS5zKwJ-I"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXQYITDVwIjD"
      },
      "source": [
        "class PseudoDataset(data.Dataset):\n",
        "    def __init__(self, images, labels, transform):\n",
        "        self.imgs = images\n",
        "        self.labels = labels\n",
        "        self.transforms = transform\n",
        "    def __getitem__(self, index):\n",
        "        img_numpy = self.imgs[index]\n",
        "        if img_numpy.shape[0]==3:\n",
        "            img_numpy = (np.transpose(img_numpy, (1,2,0)) + 1) / 2.0 * 255.0\n",
        "        elif img_numpy.shape[0] == 1:\n",
        "            img_numpy = (img_numpy[0] + 1) / 2.0 * 255.0\n",
        "        img = Image.fromarray(img_numpy.astype(np.uint8))\n",
        "        label = np.array(self.labels[index])\n",
        "        data = self.transforms(img)\n",
        "        return data, label\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "id": "yXQYITDVwIjD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71b264b6"
      },
      "source": [
        "# 新加的 fakelabel_dataset函数\n",
        "\n"
      ],
      "id": "71b264b6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcc4b5fe"
      },
      "source": [
        "\n",
        "#\n",
        "# extract image list, create new fake label\n",
        "def fakelabel_dataset(models, unlabeled_loader):\n",
        "    models['backbone'].eval()\n",
        "    all_features = torch.tensor([]).to(device)\n",
        "    # print(\"cifar10, old_dataset: \", old_dataset)\n",
        "    all_inputs = torch.tensor([])\n",
        "    all_labels = torch.tensor([])\n",
        "    # inputs, labels = old_dataset ## 新加的 是这样吗？取出dataloader里面的label\n",
        "    total_imgs = np.array([])\n",
        "    total_labels = np.array([])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        print(\">> Start clustering:\")\n",
        "\n",
        "        for (inputs, labels) in unlabeled_loader:\n",
        "            # inputs = inputs.cuda()这才是pycharm原来的\n",
        "            inputs = inputs.to(device)\n",
        "            scores, _, total_feature, features = models['backbone'](inputs)\n",
        "            # total_feature: [batch_size,512], 行，batch_size，也就是每个batch图片个数，\n",
        "            # 512是列，是resnet展平的像素feature维度数\n",
        "            all_features = torch.cat((all_features, total_feature), axis=0)\n",
        "            all_inputs = torch.cat((all_inputs, inputs.cpu()), axis=0)\n",
        "            all_labels = torch.cat((all_labels, labels.cpu()), axis=0)\n",
        "            # 从未标签数据集中选出的25000个图片的feature\n",
        "\n",
        "        input_x = np.array(all_features.cpu())\n",
        "        total_imgs = np.array(all_inputs.cpu())\n",
        "        total_labels = np.array(all_labels.cpu())\n",
        "        # input_x: shape: [BATCH_SIZE,512]\n",
        "\n",
        "        line_num, column_num = input_x.shape  # line_num：样本数量，column_num：每个样本的属性值个数\n",
        "\n",
        "        cluster_index = np.empty(line_num, dtype=int)  # line_num个样本的聚类结果\n",
        "        distance = np.empty((line_num, CLUSTER_NUMS), dtype=np.float32)\n",
        "\n",
        "        # 从line_num个数据样本中不重复地随机选择k个样本作为质心\n",
        "        cores = input_x[np.random.choice(np.arange(line_num), CLUSTER_NUMS, replace=False)]\n",
        "        iter = 0\n",
        "\n",
        "        while True:  # 迭代聚类计算,max=10000次，质心不变时break\n",
        "            iter = iter + 1\n",
        "            d = np.square(np.repeat(input_x, CLUSTER_NUMS, axis=0).reshape(line_num, CLUSTER_NUMS, column_num)\n",
        "                          - cores)\n",
        "            distance = np.sqrt(np.sum(d, axis=2))  # ndarray(line_num, k)，每个样本距离k个质心的距离，共有line_num行\n",
        "\n",
        "            index_min = np.argmin(distance, axis=1)  # 每个样本距离最近的质心索引序号 [line_num, 1]\n",
        "            if (index_min == cluster_index).all() or iter == CLUSTER_MAX_ITER:\n",
        "                break;\n",
        "\n",
        "            cluster_index[:] = index_min  # 重新分类 [line_num, 1] 这些图片分别属于哪个cluster, index\n",
        "            for i in range(CLUSTER_NUMS):  # 遍历质心集\n",
        "                if len(input_x[cluster_index == i]) != 0:\n",
        "                    items = input_x[cluster_index == i]  # 找出对应当前质心的子样本集\n",
        "                    cores[i] = np.mean(items, axis=0)  # 以子样本集的均值作为当前质心的位置\n",
        "\n",
        "        sorted_distance = np.sort(distance)  # 将矩阵的每一行升序排列\n",
        "        min_distance = np.abs(sorted_distance[:, 0].reshape(line_num, 1))  # 每个样本离自己cluster质心的距离\n",
        "\n",
        "        label_dic = {}  # 新加的，用来匹配原label和真实label\n",
        "        for i in range(CLUSTER_NUMS):  # 遍历质心集\n",
        "            if len(input_x[cluster_index == i]) != 0:\n",
        "                items = input_x[cluster_index == i]  # 找出对应当前质心的子样本集\n",
        "                item_labels = total_labels[cluster_index == i]\n",
        "                item_distances = min_distance[cluster_index == i]\n",
        "                min_index = np.argmin(item_distances)\n",
        "                label_dic[i] = item_labels[min_index]\n",
        "\n",
        "        #print(label_dic)\n",
        "        new_labels = pd.Series(cluster_index).map(label_dic)\n",
        "        print(\"cluster_index: \", cluster_index[:200])\n",
        "        fake_labels = new_labels.values  # 新加的 把cifar_unlabeled 改成假标签的dataset\n",
        "        #fake_labels = cluster_index\n",
        "\n",
        "    print(\">> Clustering Over:\")\n",
        "\n",
        "    return total_imgs, fake_labels\n"
      ],
      "id": "bcc4b5fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "411a4edc"
      },
      "source": [
        "# main.py 里面的main函数"
      ],
      "id": "411a4edc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7c84637",
        "outputId": "961a4df0-cc83-4989-d0f2-8ace86976ef5"
      },
      "source": [
        "\n",
        "##\n",
        "# Main\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    for trial in range(TRIALS):\n",
        "        # Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
        "        indices = list(range(NUM_TRAIN))\n",
        "        random.shuffle(indices)\n",
        "        START = 2 * ADDENDUM\n",
        "        labeled_set = indices[:START]\n",
        "        unlabeled_set = indices[START:]\n",
        "        semi_subset = unlabeled_set[:SUBSET]  #！！！新加的！！这儿改成了只选一部分数据\n",
        "\n",
        "        # Model\n",
        "        #backbone_net = resnet.ResNet18().cuda() #记住这才是原本pycharm里面的\n",
        "        backbone_net = ResNet18().to(device)\n",
        "        \n",
        "        models = {'backbone': backbone_net}\n",
        "\n",
        "        train_loader = DataLoader(cifar10_train, batch_size=BATCH,  # BATCH\n",
        "                                  sampler=SubsetRandomSampler(labeled_set),\n",
        "                                  pin_memory=True)\n",
        "        \n",
        "        test_loader = DataLoader(cifar10_test, batch_size=BATCH)\n",
        "        \n",
        "        total_loader = DataLoader(cifar10_unlabeled, batch_size=BATCH,\n",
        "                                          pin_memory=True)\n",
        "        '''\n",
        "        j = 0\n",
        "        for (imgs, labels) in total_loader:\n",
        "            j+=1\n",
        "            if(j<5):\n",
        "                print(\"initial labels: \", labels) #新加的，检验\n",
        "        '''\n",
        "        total_imgs, total_labels = fakelabel_dataset(models, total_loader)\n",
        "        print(\"fake_labels: \", total_labels[:100]) #新加的，检验\n",
        "  \n",
        "        cifar10_extra = PseudoDataset(total_imgs, total_labels, train_transform)\n",
        "        \n",
        "        extra_loader = DataLoader(cifar10_extra, batch_size=BATCH, #新加的\n",
        "                                  sampler=SubsetSequentialSampler(semi_subset),\n",
        "                                  pin_memory=True)\n",
        "        '''\n",
        "        i = 0\n",
        "        for (imgs, labels) in extra_loader:\n",
        "            i+=1\n",
        "            if(i<5):\n",
        "                print(\"extra_loader labels: \", labels) #新加的，检验\n",
        "        '''\n",
        "        dataloaders = {'train': train_loader, 'test': test_loader, 'extra': extra_loader}  #新加的\n",
        "\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "        # Active learning cycles\n",
        "        for cycle in range(CYCLES):\n",
        "            # Loss, criterion and scheduler (re)initialization\n",
        "            criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "            optim_backbone = optim.SGD(models['backbone'].parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WDECAY)\n",
        "\n",
        "            sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=MILESTONES)\n",
        "\n",
        "            optimizers = {'backbone': optim_backbone}\n",
        "            schedulers = {'backbone': sched_backbone}\n",
        "\n",
        "            # Training and test\n",
        "            train(models, criterion, optimizers, schedulers, dataloaders, EPOCH, EPOCHL, cycle)\n",
        "            acc = test(models, dataloaders, mode='test')\n",
        "            print('Trial {}/{} || Cycle {}/{} || Label set size {}: Test acc {}'.format(trial + 1, TRIALS, cycle + 1,\n",
        "                                                                                        CYCLES, len(labeled_set), acc),\n",
        "                                                                                         flush=True)\n",
        "\n",
        "            ##\n",
        "            #  Update the labeled dataset via loss prediction-based uncertainty measurement\n",
        "\n",
        "            # Randomly sample 10000 unlabeled data points\n",
        "            random.shuffle(unlabeled_set)\n",
        "            subset = unlabeled_set[:SUBSET]\n",
        "\n",
        "            # Create unlabeled dataloader for the unlabeled subset\n",
        "            unlabeled_loader = DataLoader(cifar10_unlabeled, batch_size=BATCH,\n",
        "                                          sampler=SubsetSequentialSampler(subset),\n",
        "                                          # more convenient if we maintain the order of subset\n",
        "                                          pin_memory=True)\n",
        "\n",
        "            # Measure uncertainty of each data points in the subset\n",
        "            uncertainty = get_uncertainty(models, unlabeled_loader)\n",
        "            #print(\"main uncertainty: \", uncertainty)\n",
        "            uncertainty = uncertainty.T\n",
        "            # Index in ascending order\n",
        "            arg = np.argsort(uncertainty).numpy().tolist()\n",
        "            #print(\"main arg: \", arg) \n",
        "            \n",
        "            # Update the labeled dataset and the unlabeled dataset, respectively\n",
        "            labeled_set += list(torch.tensor(subset)[arg][-ADDENDUM:].numpy())  # select largest loss\n",
        "            unlabeled_set = list(torch.tensor(subset)[arg][:-ADDENDUM].numpy()) + unlabeled_set[SUBSET:]\n",
        "\n",
        "            # labeled_set += list(torch.tensor(subset)[arg][:ADDENDUM].numpy())  # select smallest influence\n",
        "            # unlabeled_set = list(torch.tensor(subset)[arg][ADDENDUM:].numpy()) + unlabeled_set[SUBSET:]\n",
        "\n",
        "            # Create a new dataloader for the updated labeled dataset\n",
        "            dataloaders['train'] = DataLoader(cifar10_train, batch_size=BATCH,  # BATCH\n",
        "                                              sampler=SubsetRandomSampler(labeled_set),\n",
        "                                              pin_memory=True)\n",
        "            \n",
        "            total_imgs, total_labels = fakelabel_dataset(models, total_loader) #这里又一次聚类\n",
        "            print(\"fake_labels: \", total_labels[:100]) #新加的，检验\n",
        "  \n",
        "            cifar10_extra = PseudoDataset(total_imgs, total_labels, train_transform)\n",
        "\n",
        "            extra_subset = unlabeled_set[:SUBSET] #这儿新加的！！\n",
        "            dataloaders['extra'] = DataLoader(cifar10_extra, batch_size=BATCH,  # BATCH\n",
        "                                              sampler=SubsetRandomSampler(extra_subset),\n",
        "                                              pin_memory=True)   #新加的\n",
        "\n",
        "        # Save a checkpoint\n",
        "        torch.save({\n",
        "            'trial': trial + 1,\n",
        "            'state_dict_backbone': models['backbone'].state_dict()\n",
        "            # 'state_dict_module': models['module'].state_dict()\n",
        "        },\n",
        "            './cifar10/train/weights/active_resnet18_cifar10_trial{}.pth'.format(trial))\n",
        "\n",
        "        print('---------------------------Current Trial is done-----------------------------',flush=True)\n"
      ],
      "id": "e7c84637",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Start clustering:\n",
            "cluster_index:  [5 9 8 6 0 6 0 8 9 6 5 8 9 4 5 9 8 5 0 5 9 4 6 5 5 7 5 6 5 2 3 9 6 0 7 0 7\n",
            " 5 9 3 5 2 3 0 5 6 0 3 9 1 5 0 6 9 6 8 6 5 3 0 0 7 7 0 0 9 5 8 6 3 3 6 0 6\n",
            " 7 3 9 5 0 1 3 5 3 7 0 4 5 5 6 7 0 5 9 0 0 2 0 6 5 4 2 0 0 4 4 2 7 5 9 9 7\n",
            " 7 0 0 0 2 7 3 9 2 7 3 8 9 5 8 9 2 6 7 3 1 5 0 4 7 6 9 9 0 8 3 6 5 9 6 0 0\n",
            " 8 3 5 3 0 7 9 9 3 5 8 5 1 9 4 6 6 0 0 5 6 0 5 7 8 5 0 5 5 5 6 6 5 5 7 0 6\n",
            " 2 0 5 2 7 9 1 2 7 9 3 5 3 4 7]\n",
            ">> Clustering Over:\n",
            "fake_labels:  [3. 0. 7. 3. 9. 3. 9. 7. 0. 3. 3. 7. 0. 5. 3. 0. 7. 3. 9. 3. 0. 5. 3. 3.\n",
            " 3. 0. 3. 3. 3. 2. 4. 0. 3. 9. 0. 9. 0. 3. 0. 4. 3. 2. 4. 9. 3. 3. 9. 4.\n",
            " 0. 9. 3. 9. 3. 0. 3. 7. 3. 3. 4. 9. 9. 0. 0. 9. 9. 0. 3. 7. 3. 4. 4. 3.\n",
            " 9. 3. 0. 4. 0. 3. 9. 9. 4. 3. 4. 0. 9. 5. 3. 3. 3. 0. 9. 3. 0. 9. 9. 2.\n",
            " 9. 3. 3. 5.]\n",
            ">> Train a Model...\n",
            "Cycle: 1 Epoch: 1 --- Val Acc: 16.860 \t Best Acc: 16.860\n",
            "Cycle: 1 Epoch: 21 --- Val Acc: 36.360 \t Best Acc: 36.360\n",
            "Cycle: 1 Epoch: 41 --- Val Acc: 55.270 \t Best Acc: 55.270\n",
            "Cycle: 1 Epoch: 61 --- Val Acc: 58.310 \t Best Acc: 58.310\n",
            "Cycle: 1 Epoch: 81 --- Val Acc: 57.720 \t Best Acc: 58.310\n",
            "Cycle: 1 Epoch: 101 --- Val Acc: 60.360 \t Best Acc: 60.360\n",
            "Cycle: 1 Epoch: 121 --- Val Acc: 65.000 \t Best Acc: 65.000\n",
            "Cycle: 1 Epoch: 141 --- Val Acc: 50.360 \t Best Acc: 65.000\n",
            "Cycle: 1 Epoch: 161 --- Val Acc: 69.380 \t Best Acc: 69.380\n",
            "Cycle: 1 Epoch: 181 --- Val Acc: 71.140 \t Best Acc: 71.140\n",
            "Cycle: 1 Epoch: 200 --- Val Acc: 70.890 \t Best Acc: 71.140\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 1/7 || Label set size 5000: Test acc 70.89\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            ">> Start clustering:\n",
            "cluster_index:  [8 6 6 9 4 4 2 0 3 5 9 0 0 2 6 6 6 5 8 2 9 7 8 5 5 8 5 1 9 5 7 6 4 5 9 9 5\n",
            " 0 1 3 1 2 9 0 4 4 4 3 2 7 6 5 0 7 2 2 1 2 9 1 4 4 3 2 4 4 9 6 0 3 1 4 1 6\n",
            " 2 4 6 7 5 4 5 5 9 5 2 0 9 0 6 2 2 5 7 7 5 8 4 4 9 4 2 2 4 8 8 4 3 5 2 6 6\n",
            " 3 4 0 0 7 7 8 6 4 2 2 6 7 8 8 4 3 5 7 2 0 1 0 4 3 4 4 2 3 7 5 2 2 2 9 5 6\n",
            " 1 9 5 1 0 9 1 3 5 1 9 2 4 3 2 0 8 8 6 2 4 5 3 2 0 5 0 9 4 1 0 5 2 1 1 1 4\n",
            " 3 6 8 6 3 3 7 3 3 2 5 2 5 5 3]\n",
            ">> Clustering Over:\n",
            "fake_labels:  [6. 9. 9. 4. 1. 1. 6. 7. 8. 3. 4. 7. 7. 6. 9. 9. 9. 3. 6. 6. 4. 0. 6. 3.\n",
            " 3. 6. 3. 3. 4. 3. 0. 9. 1. 3. 4. 4. 3. 7. 3. 8. 3. 6. 4. 7. 1. 1. 1. 8.\n",
            " 6. 0. 9. 3. 7. 0. 6. 6. 3. 6. 4. 3. 1. 1. 8. 6. 1. 1. 4. 9. 7. 8. 3. 1.\n",
            " 3. 9. 6. 1. 9. 0. 3. 1. 3. 3. 4. 3. 6. 7. 4. 7. 9. 6. 6. 3. 0. 0. 3. 6.\n",
            " 1. 1. 4. 1.]\n",
            ">> Train a Model...\n",
            "Cycle: 2 Epoch: 1 --- Val Acc: 63.730 \t Best Acc: 63.730\n",
            "Cycle: 2 Epoch: 21 --- Val Acc: 64.840 \t Best Acc: 64.840\n",
            "Cycle: 2 Epoch: 41 --- Val Acc: 64.700 \t Best Acc: 64.840\n",
            "Cycle: 2 Epoch: 61 --- Val Acc: 62.220 \t Best Acc: 64.840\n",
            "Cycle: 2 Epoch: 81 --- Val Acc: 64.880 \t Best Acc: 64.880\n",
            "Cycle: 2 Epoch: 101 --- Val Acc: 66.620 \t Best Acc: 66.620\n",
            "Cycle: 2 Epoch: 121 --- Val Acc: 65.980 \t Best Acc: 66.620\n",
            "Cycle: 2 Epoch: 141 --- Val Acc: 64.210 \t Best Acc: 66.620\n",
            "Cycle: 2 Epoch: 161 --- Val Acc: 74.780 \t Best Acc: 74.780\n",
            "Cycle: 2 Epoch: 181 --- Val Acc: 76.270 \t Best Acc: 76.270\n",
            "Cycle: 2 Epoch: 200 --- Val Acc: 76.860 \t Best Acc: 76.860\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 2/7 || Label set size 7500: Test acc 76.86\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            ">> Start clustering:\n",
            "cluster_index:  [7 3 3 5 0 0 4 9 2 1 5 9 9 4 3 3 3 1 4 4 1 1 7 7 1 7 1 6 5 1 8 3 0 1 5 5 1\n",
            " 9 6 1 6 4 5 9 0 0 0 4 4 8 3 1 9 3 4 4 6 4 5 1 0 0 2 4 0 0 5 3 9 2 6 3 7 9\n",
            " 1 8 3 8 1 0 6 1 5 6 9 9 5 9 3 5 4 1 2 8 0 7 0 0 5 0 2 4 1 7 7 0 2 6 4 3 3\n",
            " 2 0 1 9 8 8 7 3 0 4 4 3 4 7 7 0 3 6 8 4 9 7 9 0 2 0 0 4 2 3 1 1 7 4 5 1 3\n",
            " 6 5 1 6 9 5 6 2 1 6 5 1 0 2 5 9 7 8 3 4 0 1 2 4 9 6 1 5 0 6 9 8 4 9 6 6 0\n",
            " 8 3 7 3 6 2 4 2 2 4 6 4 1 6 8]\n",
            ">> Clustering Over:\n",
            "fake_labels:  [6. 9. 9. 4. 1. 1. 2. 7. 8. 1. 4. 7. 7. 2. 9. 9. 9. 1. 2. 2. 1. 1. 6. 6.\n",
            " 1. 6. 1. 5. 4. 1. 0. 9. 1. 1. 4. 4. 1. 7. 5. 1. 5. 2. 4. 7. 1. 1. 1. 2.\n",
            " 2. 0. 9. 1. 7. 9. 2. 2. 5. 2. 4. 1. 1. 1. 8. 2. 1. 1. 4. 9. 7. 8. 5. 9.\n",
            " 6. 7. 1. 0. 9. 0. 1. 1. 5. 1. 4. 5. 7. 7. 4. 7. 9. 4. 2. 1. 8. 0. 1. 6.\n",
            " 1. 1. 4. 1.]\n",
            ">> Train a Model...\n",
            "Cycle: 3 Epoch: 1 --- Val Acc: 54.450 \t Best Acc: 54.450\n",
            "Cycle: 3 Epoch: 21 --- Val Acc: 71.840 \t Best Acc: 71.840\n",
            "Cycle: 3 Epoch: 41 --- Val Acc: 71.460 \t Best Acc: 71.840\n",
            "Cycle: 3 Epoch: 61 --- Val Acc: 70.870 \t Best Acc: 71.840\n",
            "Cycle: 3 Epoch: 81 --- Val Acc: 72.850 \t Best Acc: 72.850\n",
            "Cycle: 3 Epoch: 101 --- Val Acc: 69.250 \t Best Acc: 72.850\n",
            "Cycle: 3 Epoch: 121 --- Val Acc: 75.380 \t Best Acc: 75.380\n",
            "Cycle: 3 Epoch: 141 --- Val Acc: 73.510 \t Best Acc: 75.380\n",
            "Cycle: 3 Epoch: 161 --- Val Acc: 78.990 \t Best Acc: 78.990\n",
            "Cycle: 3 Epoch: 181 --- Val Acc: 80.860 \t Best Acc: 80.860\n",
            "Cycle: 3 Epoch: 200 --- Val Acc: 81.500 \t Best Acc: 81.500\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 3/7 || Label set size 10000: Test acc 81.5\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            ">> Start clustering:\n",
            "cluster_index:  [4 6 6 3 8 8 2 9 0 1 3 9 9 2 6 6 0 1 2 4 3 5 4 4 1 4 1 5 3 1 7 6 8 1 3 3 1\n",
            " 9 1 1 5 2 2 9 8 8 8 2 2 7 6 1 9 6 2 2 5 2 3 1 8 8 0 2 8 8 3 6 9 0 5 6 4 7\n",
            " 1 8 6 7 1 8 5 5 3 5 3 1 3 9 6 3 1 1 0 7 8 4 8 8 3 8 0 1 6 4 4 8 0 5 2 6 6\n",
            " 0 8 9 9 7 7 4 6 8 2 2 6 2 4 4 8 6 1 7 2 9 5 9 8 0 8 8 2 0 6 1 1 4 2 3 5 6\n",
            " 5 3 1 5 9 3 5 0 1 1 3 1 8 0 3 9 4 7 6 2 8 1 0 2 9 5 1 3 8 5 9 7 3 9 5 5 8\n",
            " 7 6 4 6 1 7 9 0 0 4 5 2 1 5 7]\n",
            ">> Clustering Over:\n",
            "fake_labels:  [6. 9. 9. 4. 1. 1. 2. 7. 8. 3. 4. 7. 7. 2. 9. 9. 8. 3. 2. 6. 4. 3. 6. 6.\n",
            " 3. 6. 3. 3. 4. 3. 0. 9. 1. 3. 4. 4. 3. 7. 3. 3. 3. 2. 2. 7. 1. 1. 1. 2.\n",
            " 2. 0. 9. 3. 7. 9. 2. 2. 3. 2. 4. 3. 1. 1. 8. 2. 1. 1. 4. 9. 7. 8. 3. 9.\n",
            " 6. 0. 3. 1. 9. 0. 3. 1. 3. 3. 4. 3. 4. 3. 4. 7. 9. 4. 3. 3. 8. 0. 1. 6.\n",
            " 1. 1. 4. 1.]\n",
            ">> Train a Model...\n",
            "Cycle: 4 Epoch: 1 --- Val Acc: 72.980 \t Best Acc: 72.980\n",
            "Cycle: 4 Epoch: 21 --- Val Acc: 77.770 \t Best Acc: 77.770\n",
            "Cycle: 4 Epoch: 41 --- Val Acc: 77.290 \t Best Acc: 77.770\n",
            "Cycle: 4 Epoch: 61 --- Val Acc: 62.490 \t Best Acc: 77.770\n",
            "Cycle: 4 Epoch: 81 --- Val Acc: 73.740 \t Best Acc: 77.770\n",
            "Cycle: 4 Epoch: 101 --- Val Acc: 70.300 \t Best Acc: 77.770\n",
            "Cycle: 4 Epoch: 121 --- Val Acc: 79.220 \t Best Acc: 79.220\n",
            "Cycle: 4 Epoch: 141 --- Val Acc: 73.240 \t Best Acc: 79.220\n",
            "Cycle: 4 Epoch: 161 --- Val Acc: 82.760 \t Best Acc: 82.760\n",
            "Cycle: 4 Epoch: 181 --- Val Acc: 82.540 \t Best Acc: 82.760\n",
            "Cycle: 4 Epoch: 200 --- Val Acc: 82.270 \t Best Acc: 82.760\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 4/7 || Label set size 12500: Test acc 82.27\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            ">> Start clustering:\n",
            "cluster_index:  [1 6 6 8 9 9 7 4 2 3 8 4 4 7 6 6 6 3 7 1 8 3 1 1 7 1 3 3 8 3 5 2 9 3 8 7 3\n",
            " 4 3 3 0 7 7 4 9 9 9 2 7 5 6 3 4 6 7 7 0 7 8 3 9 9 2 7 9 9 8 6 4 2 3 6 1 4\n",
            " 3 9 6 3 3 9 0 0 8 0 8 4 8 4 6 7 7 3 2 5 9 1 9 9 8 9 3 3 6 1 1 9 2 0 7 6 6\n",
            " 5 9 4 4 5 5 1 6 9 7 7 6 7 1 1 9 6 0 5 7 4 1 4 9 2 9 9 7 2 9 3 3 1 7 8 0 6\n",
            " 0 8 3 0 4 8 0 2 3 3 8 3 9 2 8 4 1 5 6 0 9 3 2 7 4 0 3 8 9 0 4 5 8 0 0 0 9\n",
            " 5 6 1 6 0 2 4 2 2 7 0 7 3 0 5]\n",
            ">> Clustering Over:\n",
            "fake_labels:  [6. 9. 9. 4. 1. 1. 2. 7. 8. 3. 4. 7. 7. 2. 9. 9. 9. 3. 2. 6. 4. 3. 6. 6.\n",
            " 2. 6. 3. 3. 4. 3. 0. 8. 1. 3. 4. 2. 3. 7. 3. 3. 5. 2. 2. 7. 1. 1. 1. 8.\n",
            " 2. 0. 9. 3. 7. 9. 2. 2. 5. 2. 4. 3. 1. 1. 8. 2. 1. 1. 4. 9. 7. 8. 3. 9.\n",
            " 6. 7. 3. 1. 9. 3. 3. 1. 5. 5. 4. 5. 4. 7. 4. 7. 9. 2. 2. 3. 8. 0. 1. 6.\n",
            " 1. 1. 4. 1.]\n",
            ">> Train a Model...\n",
            "Cycle: 5 Epoch: 1 --- Val Acc: 75.520 \t Best Acc: 75.520\n",
            "Cycle: 5 Epoch: 21 --- Val Acc: 69.090 \t Best Acc: 75.520\n",
            "Cycle: 5 Epoch: 41 --- Val Acc: 73.690 \t Best Acc: 75.520\n",
            "Cycle: 5 Epoch: 61 --- Val Acc: 78.500 \t Best Acc: 78.500\n",
            "Cycle: 5 Epoch: 81 --- Val Acc: 71.400 \t Best Acc: 78.500\n",
            "Cycle: 5 Epoch: 101 --- Val Acc: 75.020 \t Best Acc: 78.500\n",
            "Cycle: 5 Epoch: 121 --- Val Acc: 60.550 \t Best Acc: 78.500\n",
            "Cycle: 5 Epoch: 141 --- Val Acc: 76.970 \t Best Acc: 78.500\n",
            "Cycle: 5 Epoch: 161 --- Val Acc: 82.840 \t Best Acc: 82.840\n",
            "Cycle: 5 Epoch: 181 --- Val Acc: 83.550 \t Best Acc: 83.550\n",
            "Cycle: 5 Epoch: 200 --- Val Acc: 83.730 \t Best Acc: 83.730\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 5/7 || Label set size 15000: Test acc 83.73\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            ">> Start clustering:\n",
            "cluster_index:  [3 0 0 4 1 1 2 9 5 3 4 2 9 2 0 0 0 3 2 0 4 3 0 0 3 6 3 6 4 7 7 0 1 3 4 7 3\n",
            " 9 3 3 6 2 2 9 1 1 1 2 2 7 0 6 9 0 2 2 6 2 4 3 1 1 5 2 1 1 4 0 9 5 3 0 0 9\n",
            " 4 1 0 7 3 1 3 6 4 6 4 9 4 9 0 4 2 3 8 7 1 0 1 1 4 1 8 3 0 0 0 1 5 3 2 0 0\n",
            " 8 1 9 9 7 7 0 0 1 2 3 0 2 0 0 1 0 3 7 2 9 3 9 1 5 1 1 2 5 1 3 3 0 2 4 0 0\n",
            " 6 4 3 6 9 4 6 5 3 6 4 3 1 5 4 9 0 7 0 6 1 3 8 2 9 6 3 4 1 6 9 7 4 9 6 6 1\n",
            " 7 0 0 0 7 8 9 5 8 2 6 2 3 6 7]\n",
            ">> Clustering Over:\n",
            "fake_labels:  [3. 9. 9. 4. 1. 1. 2. 7. 8. 3. 4. 2. 7. 2. 9. 9. 9. 3. 2. 9. 4. 3. 9. 9.\n",
            " 3. 5. 3. 5. 4. 0. 0. 9. 1. 3. 4. 0. 3. 7. 3. 3. 5. 2. 2. 7. 1. 1. 1. 2.\n",
            " 2. 0. 9. 5. 7. 9. 2. 2. 5. 2. 4. 3. 1. 1. 8. 2. 1. 1. 4. 9. 7. 8. 3. 9.\n",
            " 9. 7. 4. 1. 9. 0. 3. 1. 3. 5. 4. 5. 4. 7. 4. 7. 9. 4. 2. 3. 8. 0. 1. 9.\n",
            " 1. 1. 4. 1.]\n",
            ">> Train a Model...\n",
            "Cycle: 6 Epoch: 1 --- Val Acc: 68.360 \t Best Acc: 68.360\n",
            "Cycle: 6 Epoch: 21 --- Val Acc: 70.910 \t Best Acc: 70.910\n",
            "Cycle: 6 Epoch: 41 --- Val Acc: 78.040 \t Best Acc: 78.040\n",
            "Cycle: 6 Epoch: 61 --- Val Acc: 66.440 \t Best Acc: 78.040\n",
            "Cycle: 6 Epoch: 81 --- Val Acc: 76.900 \t Best Acc: 78.040\n",
            "Cycle: 6 Epoch: 101 --- Val Acc: 74.510 \t Best Acc: 78.040\n",
            "Cycle: 6 Epoch: 121 --- Val Acc: 76.280 \t Best Acc: 78.040\n",
            "Cycle: 6 Epoch: 141 --- Val Acc: 75.810 \t Best Acc: 78.040\n",
            "Cycle: 6 Epoch: 161 --- Val Acc: 83.850 \t Best Acc: 83.850\n",
            "Cycle: 6 Epoch: 181 --- Val Acc: 85.070 \t Best Acc: 85.070\n",
            "Cycle: 6 Epoch: 200 --- Val Acc: 85.970 \t Best Acc: 85.970\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 6/7 || Label set size 17500: Test acc 85.97\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            ">> Start clustering:\n",
            "cluster_index:  [5 8 8 3 7 7 6 4 2 9 3 4 4 6 8 8 2 9 6 5 3 9 5 5 6 0 9 1 3 7 7 8 7 9 3 7 9\n",
            " 4 0 9 1 6 6 4 7 7 7 2 6 7 8 1 4 8 6 6 1 6 3 9 7 7 2 6 7 7 3 8 4 2 1 8 5 4\n",
            " 0 7 8 7 0 7 0 1 3 1 3 4 3 4 8 3 6 9 2 7 7 5 7 7 3 7 2 9 8 5 5 7 2 0 6 8 2\n",
            " 0 7 4 4 7 7 5 8 7 6 6 8 6 5 5 7 8 1 7 6 4 5 4 7 2 7 7 6 2 7 9 0 5 6 3 1 8\n",
            " 1 3 9 0 4 3 1 2 0 0 3 0 7 2 3 4 5 7 0 1 7 9 2 6 4 1 9 3 7 1 4 7 3 4 1 1 7\n",
            " 7 8 5 8 7 2 4 2 2 6 1 6 9 1 7]\n",
            ">> Clustering Over:\n",
            "fake_labels:  [6. 9. 9. 4. 1. 1. 2. 7. 8. 3. 4. 7. 7. 2. 9. 9. 8. 3. 2. 6. 4. 3. 6. 6.\n",
            " 2. 2. 3. 5. 4. 1. 1. 9. 1. 3. 4. 1. 3. 7. 2. 3. 5. 2. 2. 7. 1. 1. 1. 8.\n",
            " 2. 1. 9. 5. 7. 9. 2. 2. 5. 2. 4. 3. 1. 1. 8. 2. 1. 1. 4. 9. 7. 8. 5. 9.\n",
            " 6. 7. 2. 1. 9. 1. 2. 1. 2. 5. 4. 5. 4. 7. 4. 7. 9. 4. 2. 3. 8. 1. 1. 6.\n",
            " 1. 1. 4. 1.]\n",
            ">> Train a Model...\n",
            "Cycle: 7 Epoch: 1 --- Val Acc: 63.170 \t Best Acc: 63.170\n",
            "Cycle: 7 Epoch: 21 --- Val Acc: 69.050 \t Best Acc: 69.050\n",
            "Cycle: 7 Epoch: 41 --- Val Acc: 74.490 \t Best Acc: 74.490\n",
            "Cycle: 7 Epoch: 61 --- Val Acc: 68.660 \t Best Acc: 74.490\n",
            "Cycle: 7 Epoch: 81 --- Val Acc: 73.870 \t Best Acc: 74.490\n",
            "Cycle: 7 Epoch: 101 --- Val Acc: 77.470 \t Best Acc: 77.470\n",
            "Cycle: 7 Epoch: 121 --- Val Acc: 76.470 \t Best Acc: 77.470\n",
            "Cycle: 7 Epoch: 141 --- Val Acc: 76.210 \t Best Acc: 77.470\n",
            "Cycle: 7 Epoch: 161 --- Val Acc: 82.630 \t Best Acc: 82.630\n",
            "Cycle: 7 Epoch: 181 --- Val Acc: 85.090 \t Best Acc: 85.090\n",
            "Cycle: 7 Epoch: 200 --- Val Acc: 86.090 \t Best Acc: 86.090\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 7/7 || Label set size 20000: Test acc 86.09\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            ">> Start clustering:\n",
            "cluster_index:  [9 4 4 7 3 3 2 0 6 8 7 1 0 2 4 4 4 8 2 9 7 8 9 9 2 9 8 5 7 4 4 8 3 8 7 2 8\n",
            " 0 8 8 5 2 2 0 3 3 3 2 2 4 4 5 1 4 2 2 5 2 7 8 3 3 6 2 3 3 7 4 0 6 5 2 9 0\n",
            " 8 3 4 4 8 3 8 5 7 5 7 1 7 0 4 7 2 8 6 4 3 9 3 3 7 3 6 8 4 9 9 3 6 5 2 4 6\n",
            " 6 3 1 0 4 4 9 4 3 2 2 4 2 9 9 3 4 5 4 2 1 9 0 3 6 3 3 2 6 3 8 2 9 2 7 4 4\n",
            " 5 7 8 8 0 7 5 6 5 5 7 8 3 6 7 0 9 4 4 5 3 8 6 2 0 5 8 7 3 5 1 5 7 0 5 5 3\n",
            " 4 4 9 4 5 6 1 6 6 2 5 2 8 5 4]\n",
            ">> Clustering Over:\n",
            "fake_labels:  [6. 9. 9. 4. 1. 1. 2. 7. 8. 3. 4. 7. 7. 2. 9. 9. 9. 3. 2. 6. 4. 3. 6. 6.\n",
            " 2. 6. 3. 5. 4. 9. 9. 3. 1. 3. 4. 2. 3. 7. 3. 3. 5. 2. 2. 7. 1. 1. 1. 2.\n",
            " 2. 9. 9. 5. 7. 9. 2. 2. 5. 2. 4. 3. 1. 1. 8. 2. 1. 1. 4. 9. 7. 8. 5. 2.\n",
            " 6. 7. 3. 1. 9. 9. 3. 1. 3. 5. 4. 5. 4. 7. 4. 7. 9. 4. 2. 3. 8. 9. 1. 6.\n",
            " 1. 1. 4. 1.]\n",
            "---------------------------Current Trial is done-----------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e21adede"
      },
      "source": [
        ""
      ],
      "id": "e21adede",
      "execution_count": null,
      "outputs": []
    }
  ]
}