{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "c10_AL_code_第三版_GMM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "7Ih0CfNODItq",
        "outputId": "13f90f63-4619-430b-9ffe-a1f9e7fdc92f"
      },
      "source": [
        "'''\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "'''"
      ],
      "id": "7Ih0CfNODItq",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\\n!apt-get update -qq 2>&1 > /dev/null\\n!apt-get -y install -qq google-drive-ocamlfuse fuse\\nfrom google.colab import auth\\nauth.authenticate_user()\\nfrom oauth2client.client import GoogleCredentials\\ncreds = GoogleCredentials.get_application_default()\\nimport getpass\\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\\nvcode = getpass.getpass()\\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "08eGFWBxDb6J",
        "outputId": "0910e569-70f0-4e6b-ed10-532bb4a9ed16"
      },
      "source": [
        "'''\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "'''"
      ],
      "id": "08eGFWBxDb6J",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!mkdir -p drive\\n!google-drive-ocamlfuse drive\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wFhHPc5RKif",
        "outputId": "53a76718-3af5-45af-8fae-6b8e86bd6ae2"
      },
      "source": [
        "!pip install torchvision"
      ],
      "id": "7wFhHPc5RKif",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFsQizA6kUmx",
        "outputId": "71cefa1d-628b-444d-becb-579a38ef9d2c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "tFsQizA6kUmx",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun  3 18:05:53 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    39W / 250W |   2213MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPZGxjcmWiOG"
      },
      "source": [
        "#!pip install mxnet-cu112 # 没用着呢\n",
        "#!pip install minpy"
      ],
      "id": "qPZGxjcmWiOG",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smjt0fCkRKzZ",
        "outputId": "1a505917-ba80-4f61-fbff-d9a3681efae1"
      },
      "source": [
        "import torch \n",
        "x = torch.empty(5,3)\n",
        "print(x)"
      ],
      "id": "smjt0fCkRKzZ",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.9926e-25, 3.0831e-41, 4.4842e-44],\n",
            "        [4.4842e-44, 4.4842e-44, 4.4842e-44],\n",
            "        [4.4842e-44, 4.4842e-44, 4.7644e-44],\n",
            "        [4.7644e-44, 4.7644e-44, 1.4293e-43],\n",
            "        [0.0000e+00, 0.0000e+00, 9.1477e-41]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6694aee5"
      },
      "source": [
        "# main.py 前面import"
      ],
      "id": "6694aee5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9914fdc7"
      },
      "source": [
        "# -*- coding: UTF-8 -*-\n",
        "# Python\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "import numpy as np  #注意这里改了，改成能用GPU的numpy了\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from numpy.core._multiarray_umath import ndarray\n",
        "from sklearn.cluster import KMeans\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Torchvison\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import CIFAR100, CIFAR10\n",
        "# from influence import *\n",
        "\n",
        "# Cluster\n",
        "from sklearn.mixture import GaussianMixture\n",
        "# Utils\n",
        "# import visdom\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# Custom\n",
        "#import models.resnet as resnet\n",
        "#from config import *\n",
        "#from data.sampler import SubsetSequentialSampler\n",
        "\n",
        "# import copy\n",
        "\n",
        "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
      ],
      "id": "9914fdc7",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbe61243"
      },
      "source": [
        "# config.py"
      ],
      "id": "fbe61243"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "d29a58de",
        "outputId": "7c30d96e-c642-431e-f0dc-280a8981403e"
      },
      "source": [
        "##\n",
        "# Learning Loss for Active Learning\n",
        "NUM_TRAIN = 50000 # N\n",
        "NUM_VAL   = 50000 - NUM_TRAIN\n",
        "BATCH     = 128  # 128 # B  注意，这里本来是是128\n",
        "SUBSET    = 25000 # M  # 本来是25000\n",
        "ADDENDUM  = 2500  # K   # 本来是2500\n",
        "\n",
        "MARGIN = 1.0  # xi\n",
        "WEIGHT = 1.0  # 1.0 # lambda\n",
        "\n",
        "TRIALS = 1\n",
        "CYCLES = 7   # 本来应该是7\n",
        "\n",
        "EPOCH = 200   # 本来是200\n",
        "LR = 0.1    # 0.1 for SGD\n",
        "MILESTONES = [160]\n",
        "EPOCHL = 120 # After 120 epochs, stop the gradient from the loss prediction module propagated to the target model\n",
        "\n",
        "MOMENTUM = 0.9\n",
        "WDECAY = 5e-4\n",
        "\n",
        "CLUSTER_NUMS = 10 # 新加的，聚类的cluster 数量\n",
        "CLUSTER_MAX_ITER = 1000 # 新加的，聚类的迭代次数，本来是10000\n",
        "\n",
        "\n",
        "''' CIFAR-10 | ResNet-18 | 93.6%\n",
        "NUM_TRAIN = 50000 # N\n",
        "NUM_VAL   = 50000 - NUM_TRAIN\n",
        "BATCH     = 128 # B\n",
        "SUBSET    = NUM_TRAIN # M\n",
        "ADDENDUM  = NUM_TRAIN # K\n",
        "\n",
        "MARGIN = 1.0 # xi\n",
        "WEIGHT = 0.0 # lambda\n",
        "\n",
        "TRIALS = 1\n",
        "CYCLES = 1\n",
        "\n",
        "EPOCH = 50\n",
        "LR = 0.1\n",
        "MILESTONES = [25, 35]\n",
        "EPOCHL = 40\n",
        "\n",
        "MOMENTUM = 0.9\n",
        "WDECAY = 5e-4\n",
        "'''\n"
      ],
      "id": "d29a58de",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' CIFAR-10 | ResNet-18 | 93.6%\\nNUM_TRAIN = 50000 # N\\nNUM_VAL   = 50000 - NUM_TRAIN\\nBATCH     = 128 # B\\nSUBSET    = NUM_TRAIN # M\\nADDENDUM  = NUM_TRAIN # K\\n\\nMARGIN = 1.0 # xi\\nWEIGHT = 0.0 # lambda\\n\\nTRIALS = 1\\nCYCLES = 1\\n\\nEPOCH = 50\\nLR = 0.1\\nMILESTONES = [25, 35]\\nEPOCHL = 40\\n\\nMOMENTUM = 0.9\\nWDECAY = 5e-4\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12149227"
      },
      "source": [
        "# device设置成CPU或GPU "
      ],
      "id": "12149227"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed3207d4",
        "outputId": "00e5155e-ce2b-46b8-fc10-f006056b7545"
      },
      "source": [
        "#\n",
        "# 加一个device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.is_available())"
      ],
      "id": "ed3207d4",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad37c670"
      },
      "source": [
        "# sampler.py"
      ],
      "id": "ad37c670"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fd8a205"
      },
      "source": [
        "class SubsetSequentialSampler(torch.utils.data.Sampler):\n",
        "    r\"\"\"Samples elements sequentially from a given list of indices, without replacement.\n",
        "\n",
        "    Arguments:\n",
        "        indices (sequence): a sequence of indices\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, indices):\n",
        "        self.indices = indices\n",
        "\n",
        "    def __iter__(self):\n",
        "        return (self.indices[i] for i in range(len(self.indices)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n"
      ],
      "id": "8fd8a205",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08c18965"
      },
      "source": [
        "\n",
        "# main.py 里面的data"
      ],
      "id": "08c18965"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdbe408f",
        "outputId": "4927aef8-2112-4880-c1c4-73dabad16194"
      },
      "source": [
        "##\n",
        "# Data\n",
        "train_transform = T.Compose([\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomCrop(size=32, padding=4),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "cifar10_train = CIFAR10('../cifar10', train=True, download=True, transform=train_transform)  # specify data path here\n",
        "cifar10_unlabeled = CIFAR10('../cifar10', train=True, download=True, transform=test_transform)\n",
        "cifar10_test = CIFAR10('../cifar10', train=False, download=True, transform=test_transform)\n",
        "\n",
        "##\n",
        "# Train Utils\n",
        "iters = 0\n"
      ],
      "id": "cdbe408f",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91114054"
      },
      "source": [
        "# resnet.py"
      ],
      "id": "91114054"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36b3fb7f"
      },
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "        self.linear1 = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out1 = self.layer1(out)\n",
        "        out2 = self.layer2(out1)\n",
        "        out3 = self.layer3(out2)\n",
        "        out4 = self.layer4(out3)\n",
        "        out5 = F.avg_pool2d(out4, 4)\n",
        "        out5 = out5.view(out5.size(0), -1)     # [128, 512]\n",
        "        out = self.linear(out5)\n",
        "        out_cons = self.linear1(out5)\n",
        "\n",
        "        return out, out_cons, out5, [out1, out2, out3, out4]\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet18_student():\n",
        "    return ResNet(BasicBlock, [1,1,1,1])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3,8,36,3])\n",
        "\n"
      ],
      "id": "36b3fb7f",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "280a2c17"
      },
      "source": [
        "# main.py 里面的train_epoch函数"
      ],
      "id": "280a2c17"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e52d13f"
      },
      "source": [
        "#\n",
        "def train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss):\n",
        "    models['backbone'].train()\n",
        "    global iters\n",
        "\n",
        "    for data in dataloaders['train']:\n",
        "        #inputs = data[0].cuda()这是pycharm原来的\n",
        "        inputs = data[0].to(device)\n",
        "        #labels = data[1].cuda()这是pycharm原来的\n",
        "        labels = data[1].to(device)\n",
        "        iters += 1\n",
        "\n",
        "        optimizers['backbone'].zero_grad()\n",
        "\n",
        "        scores, _, _, features_list = models['backbone'](inputs)\n",
        "        target_loss = criterion(scores, labels)\n",
        "\n",
        "        m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)\n",
        "        loss = m_backbone_loss\n",
        "        loss.backward()\n",
        "        optimizers['backbone'].step()\n"
      ],
      "id": "5e52d13f",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8427241c"
      },
      "source": [
        "# main.py 里面的test函数"
      ],
      "id": "8427241c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0a4bf72"
      },
      "source": [
        "#\n",
        "def test(models, dataloaders, mode='val'):\n",
        "    assert mode == 'val' or mode == 'test'\n",
        "    models['backbone'].eval()\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for (inputs, labels) in dataloaders[mode]:\n",
        "            #inputs = inputs.cuda()这才是prcharm原来的\n",
        "            inputs = inputs.to(device)\n",
        "            #labels = labels.cuda()这才是prcharm原来的\n",
        "            labels = labels.to(device)\n",
        "            scores, _, _, _ = models['backbone'](inputs)\n",
        "            _, preds = torch.max(scores.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "    return 100 * correct / total"
      ],
      "id": "e0a4bf72",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c901cd5a"
      },
      "source": [
        "# main.py里面的train函数"
      ],
      "id": "c901cd5a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1e24b49"
      },
      "source": [
        "#\n",
        "def train(models, criterion, optimizers, schedulers, dataloaders, num_epochs, epoch_loss, cycle):\n",
        "    print('>> Train a Model...')\n",
        "    best_acc = 0.\n",
        "    checkpoint_dir = os.path.join('./cifar10', 'train', 'weights')\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss)\n",
        "        schedulers['backbone'].step()\n",
        "\n",
        "        # Save a checkpoint\n",
        "        if epoch % 20 == 0 or epoch == EPOCH - 1:\n",
        "            acc = test(models, dataloaders, 'test')\n",
        "            if best_acc < acc:\n",
        "                best_acc = acc\n",
        "                torch.save({\n",
        "                    'epoch': epoch + 1,\n",
        "                    'state_dict_backbone': models['backbone'].state_dict()\n",
        "                    # 'state_dict_module': models['module'].state_dict()\n",
        "                },\n",
        "                    '%s/active_resnet18_cifar10.pth' % (checkpoint_dir))\n",
        "            print('Cycle:', cycle + 1, 'Epoch:', epoch + 1, \"---\", 'Val Acc: {:.3f} \\t Best Acc: {:.3f}'.format(acc, best_acc),flush=True)\n",
        "            \n",
        "    print('>> Finished.')\n"
      ],
      "id": "e1e24b49",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d04daa2b"
      },
      "source": [
        "# GMM 高斯混合模型"
      ],
      "id": "d04daa2b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "51a5405d",
        "outputId": "dc1e72ae-7611-474c-b381-f632dccdae61"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ----------------------------------------------------\n",
        "# Copyright (c) 2017, Wray Zheng. All Rights Reserved.\n",
        "# Distributed under the BSD License.\n",
        "# ----------------------------------------------------\n",
        "'''\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "DEBUG = True\n",
        "\n",
        "######################################################\n",
        "# 调试输出函数\n",
        "# 由全局变量 DEBUG 控制输出\n",
        "######################################################\n",
        "def debug(*args, **kwargs):\n",
        "    global DEBUG\n",
        "    if DEBUG:\n",
        "        print(*args, **kwargs)\n",
        "\n",
        "\n",
        "######################################################\n",
        "# 第 k 个模型的高斯分布密度函数\n",
        "# 每 i 行表示第 i 个样本在各模型中的出现概率\n",
        "# 返回一维列表\n",
        "######################################################\n",
        "def phi(Y, mu_k, cov_k):\n",
        "    norm = multivariate_normal(mean=mu_k, cov=cov_k)\n",
        "    return norm.pdf(Y)\n",
        "\n",
        "\n",
        "######################################################\n",
        "# E 步：计算每个模型对样本的响应度\n",
        "# Y 为样本矩阵，每个样本一行，只有一个特征时为列向量\n",
        "# mu 为均值多维数组，每行表示一个样本各个特征的均值\n",
        "# cov 为协方差矩阵的数组，alpha 为模型响应度数组\n",
        "######################################################\n",
        "def getExpectation(Y, mu, cov, alpha):\n",
        "    # 样本数\n",
        "    N = Y.shape[0]\n",
        "    # 模型数\n",
        "    K = alpha.shape[0]\n",
        "\n",
        "    # 为避免使用单个高斯模型或样本，导致返回结果的类型不一致\n",
        "    # 因此要求样本数和模型个数必须大于1\n",
        "    assert N > 1, \"There must be more than one sample!\"\n",
        "    assert K > 1, \"There must be more than one gaussian model!\"\n",
        "\n",
        "    # 响应度矩阵，行对应样本，列对应响应度\n",
        "    gamma = np.mat(np.zeros((N, K)))\n",
        "\n",
        "    # 计算各模型中所有样本出现的概率，行对应样本，列对应模型\n",
        "    prob = np.zeros((N, K))\n",
        "    for k in range(K):\n",
        "        prob[:, k] = phi(Y, mu[k], cov[k])\n",
        "    prob = np.mat(prob)\n",
        "\n",
        "    # 计算每个模型对每个样本的响应度\n",
        "    for k in range(K):\n",
        "        gamma[:, k] = alpha[k] * prob[:, k]\n",
        "    for i in range(N):\n",
        "        gamma[i, :] /= np.sum(gamma[i, :])\n",
        "    return gamma\n",
        "\n",
        "\n",
        "######################################################\n",
        "# M 步：迭代模型参数\n",
        "# Y 为样本矩阵，gamma 为响应度矩阵\n",
        "######################################################\n",
        "def maximize(Y, gamma):\n",
        "    # 样本数和特征数\n",
        "    N, D = Y.shape\n",
        "    # 模型数\n",
        "    K = gamma.shape[1]\n",
        "\n",
        "    #初始化参数值\n",
        "    mu = np.zeros((K, D))\n",
        "    cov = []\n",
        "    alpha = np.zeros(K)\n",
        "\n",
        "    # 更新每个模型的参数\n",
        "    for k in range(K):\n",
        "        # 第 k 个模型对所有样本的响应度之和\n",
        "        Nk = np.sum(gamma[:, k])\n",
        "        # 更新 mu\n",
        "        # 对每个特征求均值\n",
        "        mu[k, :] = np.sum(np.multiply(Y, gamma[:, k]), axis=0) / Nk\n",
        "        # 更新 cov\n",
        "        cov_k = (Y - mu[k]).T * np.multiply((Y - mu[k]), gamma[:, k]) / Nk\n",
        "        cov.append(cov_k)\n",
        "        # 更新 alpha\n",
        "        alpha[k] = Nk / N\n",
        "    cov = np.array(cov)\n",
        "    return mu, cov, alpha\n",
        "\n",
        "\n",
        "######################################################\n",
        "# 数据预处理\n",
        "# 将所有数据都缩放到 0 和 1 之间\n",
        "######################################################\n",
        "def scale_data(Y):\n",
        "    # 对每一维特征分别进行缩放\n",
        "    for i in range(Y.shape[1]):\n",
        "        max_ = Y[:, i].max()\n",
        "        min_ = Y[:, i].min()\n",
        "        Y[:, i] = (Y[:, i] - min_) / (max_ - min_)\n",
        "    debug(\"Data scaled.\")\n",
        "    return Y\n",
        "\n",
        "\n",
        "######################################################\n",
        "# 初始化模型参数\n",
        "# shape 是表示样本规模的二元组，(样本数, 特征数)\n",
        "# K 表示模型个数\n",
        "######################################################\n",
        "def init_params(shape, K):\n",
        "    N, D = shape\n",
        "    mu = np.random.rand(K, D)\n",
        "    cov = np.array([np.eye(D)] * K)\n",
        "    alpha = np.array([1.0 / K] * K)\n",
        "    debug(\"Parameters initialized.\")\n",
        "    debug(\"mu:\", mu, \"cov:\", cov, \"alpha:\", alpha, sep=\"\\n\")\n",
        "    return mu, cov, alpha\n",
        "\n",
        "\n",
        "######################################################\n",
        "# 高斯混合模型 EM 算法\n",
        "# 给定样本矩阵 Y，计算模型参数\n",
        "# K 为模型个数\n",
        "# times 为迭代次数\n",
        "######################################################\n",
        "def GMM_EM(Y, K, times):\n",
        "    Y = scale_data(Y)\n",
        "    mu, cov, alpha = init_params(Y.shape, K)\n",
        "    for i in range(times):\n",
        "        gamma = getExpectation(Y, mu, cov, alpha)\n",
        "        mu, cov, alpha = maximize(Y, gamma)\n",
        "    debug(\"{sep} Result {sep}\".format(sep=\"-\" * 20))\n",
        "    debug(\"mu:\", mu, \"cov:\", cov, \"alpha:\", alpha, sep=\"\\n\")\n",
        "    return mu, cov, alpha\n",
        "'''"
      ],
      "id": "51a5405d",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom scipy.stats import multivariate_normal\\n\\nDEBUG = True\\n\\n######################################################\\n# 调试输出函数\\n# 由全局变量 DEBUG 控制输出\\n######################################################\\ndef debug(*args, **kwargs):\\n    global DEBUG\\n    if DEBUG:\\n        print(*args, **kwargs)\\n\\n\\n######################################################\\n# 第 k 个模型的高斯分布密度函数\\n# 每 i 行表示第 i 个样本在各模型中的出现概率\\n# 返回一维列表\\n######################################################\\ndef phi(Y, mu_k, cov_k):\\n    norm = multivariate_normal(mean=mu_k, cov=cov_k)\\n    return norm.pdf(Y)\\n\\n\\n######################################################\\n# E 步：计算每个模型对样本的响应度\\n# Y 为样本矩阵，每个样本一行，只有一个特征时为列向量\\n# mu 为均值多维数组，每行表示一个样本各个特征的均值\\n# cov 为协方差矩阵的数组，alpha 为模型响应度数组\\n######################################################\\ndef getExpectation(Y, mu, cov, alpha):\\n    # 样本数\\n    N = Y.shape[0]\\n    # 模型数\\n    K = alpha.shape[0]\\n\\n    # 为避免使用单个高斯模型或样本，导致返回结果的类型不一致\\n    # 因此要求样本数和模型个数必须大于1\\n    assert N > 1, \"There must be more than one sample!\"\\n    assert K > 1, \"There must be more than one gaussian model!\"\\n\\n    # 响应度矩阵，行对应样本，列对应响应度\\n    gamma = np.mat(np.zeros((N, K)))\\n\\n    # 计算各模型中所有样本出现的概率，行对应样本，列对应模型\\n    prob = np.zeros((N, K))\\n    for k in range(K):\\n        prob[:, k] = phi(Y, mu[k], cov[k])\\n    prob = np.mat(prob)\\n\\n    # 计算每个模型对每个样本的响应度\\n    for k in range(K):\\n        gamma[:, k] = alpha[k] * prob[:, k]\\n    for i in range(N):\\n        gamma[i, :] /= np.sum(gamma[i, :])\\n    return gamma\\n\\n\\n######################################################\\n# M 步：迭代模型参数\\n# Y 为样本矩阵，gamma 为响应度矩阵\\n######################################################\\ndef maximize(Y, gamma):\\n    # 样本数和特征数\\n    N, D = Y.shape\\n    # 模型数\\n    K = gamma.shape[1]\\n\\n    #初始化参数值\\n    mu = np.zeros((K, D))\\n    cov = []\\n    alpha = np.zeros(K)\\n\\n    # 更新每个模型的参数\\n    for k in range(K):\\n        # 第 k 个模型对所有样本的响应度之和\\n        Nk = np.sum(gamma[:, k])\\n        # 更新 mu\\n        # 对每个特征求均值\\n        mu[k, :] = np.sum(np.multiply(Y, gamma[:, k]), axis=0) / Nk\\n        # 更新 cov\\n        cov_k = (Y - mu[k]).T * np.multiply((Y - mu[k]), gamma[:, k]) / Nk\\n        cov.append(cov_k)\\n        # 更新 alpha\\n        alpha[k] = Nk / N\\n    cov = np.array(cov)\\n    return mu, cov, alpha\\n\\n\\n######################################################\\n# 数据预处理\\n# 将所有数据都缩放到 0 和 1 之间\\n######################################################\\ndef scale_data(Y):\\n    # 对每一维特征分别进行缩放\\n    for i in range(Y.shape[1]):\\n        max_ = Y[:, i].max()\\n        min_ = Y[:, i].min()\\n        Y[:, i] = (Y[:, i] - min_) / (max_ - min_)\\n    debug(\"Data scaled.\")\\n    return Y\\n\\n\\n######################################################\\n# 初始化模型参数\\n# shape 是表示样本规模的二元组，(样本数, 特征数)\\n# K 表示模型个数\\n######################################################\\ndef init_params(shape, K):\\n    N, D = shape\\n    mu = np.random.rand(K, D)\\n    cov = np.array([np.eye(D)] * K)\\n    alpha = np.array([1.0 / K] * K)\\n    debug(\"Parameters initialized.\")\\n    debug(\"mu:\", mu, \"cov:\", cov, \"alpha:\", alpha, sep=\"\\n\")\\n    return mu, cov, alpha\\n\\n\\n######################################################\\n# 高斯混合模型 EM 算法\\n# 给定样本矩阵 Y，计算模型参数\\n# K 为模型个数\\n# times 为迭代次数\\n######################################################\\ndef GMM_EM(Y, K, times):\\n    Y = scale_data(Y)\\n    mu, cov, alpha = init_params(Y.shape, K)\\n    for i in range(times):\\n        gamma = getExpectation(Y, mu, cov, alpha)\\n        mu, cov, alpha = maximize(Y, gamma)\\n    debug(\"{sep} Result {sep}\".format(sep=\"-\" * 20))\\n    debug(\"mu:\", mu, \"cov:\", cov, \"alpha:\", alpha, sep=\"\\n\")\\n    return mu, cov, alpha\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71b264b6"
      },
      "source": [
        "# main.py里面的get_uncertainty函数"
      ],
      "id": "71b264b6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcc4b5fe"
      },
      "source": [
        "#\n",
        "def get_uncertainty(models, unlabeled_loader):\n",
        "    models['backbone'].eval()\n",
        "    uncertainty = torch.tensor([]).to(device) #这是原来版本，聚类先不用GPU，用CPU可以用numpy\n",
        "\n",
        "    with torch.no_grad():\n",
        "        print(\">> Start clustering:\")\n",
        "        for (inputs, labels) in unlabeled_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            \n",
        "            scores, _, total_feature, features = models['backbone'](inputs)\n",
        "            # total_feature: [SUBSET,512], SUBSET是行，是batch_size，也就是图片个数，512是列，是resnet展平的像素feature个数\n",
        "\n",
        "            # TODO Use clustering to determine data uncertainty\n",
        "            input_x = np.array(total_feature.cpu())\n",
        "            gmm = GaussianMixture(n_components=10).fit(input_x)\n",
        "            posterior = torch.Tensor(gmm.predict_proba(input_x)).to(device) #后验概率\n",
        "            uncertainty = Categorical(probs=posterior).entropy()  #计算entropy混乱度，混乱度高说明样本容易被分到很多cluster,是个四不像，\n",
        "                                                                  # 我觉得四不像的值有时候反而不该选，该选最大的两个概率的差最小的，\n",
        "                                                                  #或者多个，也就是最容易被分错的\n",
        "            \n",
        "            '''\n",
        "                GMM 算法\n",
        "            \n",
        "            '''\n",
        "            '''\n",
        "            input_x = scale_data(input_x)\n",
        "            line_num, column_num = input_x.shape\n",
        "            result = np.empty(line_num, dtype=np.int)  # line_num个样本的聚类结果\n",
        "            gamma = np.empty((line_num, CLUSTER_NUMS), dtype=np.float32)  # 我加的\n",
        "            \n",
        "            iter = 0\n",
        "            \n",
        "            while line_num != 0:  # 迭代聚类计算,这是10000次，这里也可以用while True，质心不变时停止\n",
        "                iter = iter + 1\n",
        "            \n",
        "                mu, cov, alpha = init_params(input_x.shape, CLUSTER_NUMS)\n",
        "                gamma = getExpectation(input_x, mu, cov, alpha)\n",
        "                mu, cov, alpha = maximize(input_x, gamma)\n",
        "                \n",
        "                index_max = np.argmax(gamma, axis=1)  # 每个样本距离属于的质心索引序号 [line_num, 1]\n",
        "                if (index_max == result).all() or iter == CLUSTER_MAX_ITER:\n",
        "                    break;\n",
        "                #debug(\"{sep} Result {sep}\".format(sep=\"-\" * 20))\n",
        "                #debug(\"mu:\", mu, \"cov:\", cov, \"alpha:\", alpha, sep=\"\\n\")\n",
        "\n",
        "\n",
        "            sorted_gamma = np.sort(gamma)\n",
        "            two_maxgamma_differ = np.abs(sorted_gamma[:, -1].reshape(line_num, 1)\n",
        "                                         - sorted_gamma[:, -2].reshape(line_num, 1)) #方法一\n",
        "            two_maxgamma_differ_torch = torch.from_numpy(two_maxgamma_differ)\n",
        "            uncertainty = torch.cat((uncertainty, 10.00 / two_maxgamma_differ_torch), 0)\n",
        "            '''\n",
        "            \n",
        "    print(\">> Clustering Over:\")\n",
        "    return uncertainty.cpu()\n"
      ],
      "id": "bcc4b5fe",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "411a4edc"
      },
      "source": [
        "# main.py 里面的main函数"
      ],
      "id": "411a4edc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7c84637",
        "outputId": "69d0c15a-7641-483b-aa59-90d12def4a32"
      },
      "source": [
        "\n",
        "##\n",
        "# Main\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    for trial in range(TRIALS):\n",
        "        # Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
        "        indices = list(range(NUM_TRAIN))\n",
        "        random.shuffle(indices)\n",
        "        START = 2 * ADDENDUM\n",
        "        labeled_set = indices[:START]\n",
        "        unlabeled_set = indices[START:]\n",
        "\n",
        "        train_loader = DataLoader(cifar10_train, batch_size=BATCH,  # BATCH\n",
        "                                  sampler=SubsetRandomSampler(labeled_set),\n",
        "                                  pin_memory=True)\n",
        "        test_loader = DataLoader(cifar10_test, batch_size=BATCH)\n",
        "\n",
        "        dataloaders = {'train': train_loader, 'test': test_loader}\n",
        "\n",
        "        # Model\n",
        "        #backbone_net = resnet.ResNet18().cuda() #记住这才是原本pycharm里面的\n",
        "        backbone_net = ResNet18().to(device)\n",
        "        \n",
        "        models = {'backbone': backbone_net}\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "        # Active learning cycles\n",
        "        for cycle in range(CYCLES):\n",
        "            # Loss, criterion and scheduler (re)initialization\n",
        "            criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "            optim_backbone = optim.SGD(models['backbone'].parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WDECAY)\n",
        "\n",
        "            sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=MILESTONES)\n",
        "\n",
        "            optimizers = {'backbone': optim_backbone}\n",
        "            schedulers = {'backbone': sched_backbone}\n",
        "\n",
        "            # Training and test\n",
        "            train(models, criterion, optimizers, schedulers, dataloaders, EPOCH, EPOCHL, cycle)\n",
        "            acc = test(models, dataloaders, mode='test')\n",
        "            print('Trial {}/{} || Cycle {}/{} || Label set size {}: Test acc {}'.format(trial + 1, TRIALS, cycle + 1,\n",
        "                                                                                        CYCLES, len(labeled_set), acc),\n",
        "                                                                                         flush=True)\n",
        "\n",
        "            ##\n",
        "            #  Update the labeled dataset via loss prediction-based uncertainty measurement\n",
        "\n",
        "            # Randomly sample 10000 unlabeled data points\n",
        "            random.shuffle(unlabeled_set)\n",
        "            subset = unlabeled_set[:SUBSET]\n",
        "\n",
        "            # Create unlabeled dataloader for the unlabeled subset\n",
        "            unlabeled_loader = DataLoader(cifar10_unlabeled, batch_size=BATCH,\n",
        "                                          sampler=SubsetSequentialSampler(subset),\n",
        "                                          # more convenient if we maintain the order of subset\n",
        "                                          pin_memory=True)\n",
        "\n",
        "            # Measure uncertainty of each data points in the subset\n",
        "            uncertainty = get_uncertainty(models, unlabeled_loader)\n",
        "            #print(\"main uncertainty: \", uncertainty)\n",
        "            uncertainty = uncertainty.T\n",
        "            # Index in ascending order\n",
        "            arg = np.argsort(uncertainty).numpy().tolist()\n",
        "            #print(\"main arg: \", arg) \n",
        "            \n",
        "            # Update the labeled dataset and the unlabeled dataset, respectively\n",
        "            labeled_set += list(torch.tensor(subset)[arg][-ADDENDUM:].numpy())  # select largest loss\n",
        "            unlabeled_set = list(torch.tensor(subset)[arg][:-ADDENDUM].numpy()) + unlabeled_set[SUBSET:]\n",
        "\n",
        "            # labeled_set += list(torch.tensor(subset)[arg][:ADDENDUM].numpy())  # select smallest influence\n",
        "            # unlabeled_set = list(torch.tensor(subset)[arg][ADDENDUM:].numpy()) + unlabeled_set[SUBSET:]\n",
        "\n",
        "            # Create a new dataloader for the updated labeled dataset\n",
        "            dataloaders['train'] = DataLoader(cifar10_train, batch_size=BATCH,  # BATCH\n",
        "                                              sampler=SubsetRandomSampler(labeled_set),\n",
        "                                              pin_memory=True)\n",
        "\n",
        "        # Save a checkpoint\n",
        "        torch.save({\n",
        "            'trial': trial + 1,\n",
        "            'state_dict_backbone': models['backbone'].state_dict()\n",
        "            # 'state_dict_module': models['module'].state_dict()\n",
        "        },\n",
        "            './cifar10/train/weights/active_resnet18_cifar10_trial{}.pth'.format(trial))\n",
        "\n",
        "        print('---------------------------Current Trial is done-----------------------------',flush=True)\n"
      ],
      "id": "e7c84637",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Train a Model...\n",
            "Cycle: 1 Epoch: 1 --- Val Acc: 10.990 \t Best Acc: 10.990\n",
            "Cycle: 1 Epoch: 21 --- Val Acc: 44.060 \t Best Acc: 44.060\n",
            "Cycle: 1 Epoch: 41 --- Val Acc: 55.710 \t Best Acc: 55.710\n",
            "Cycle: 1 Epoch: 61 --- Val Acc: 63.450 \t Best Acc: 63.450\n",
            "Cycle: 1 Epoch: 81 --- Val Acc: 68.310 \t Best Acc: 68.310\n",
            "Cycle: 1 Epoch: 101 --- Val Acc: 69.060 \t Best Acc: 69.060\n",
            "Cycle: 1 Epoch: 121 --- Val Acc: 63.240 \t Best Acc: 69.060\n",
            "Cycle: 1 Epoch: 141 --- Val Acc: 69.940 \t Best Acc: 69.940\n",
            "Cycle: 1 Epoch: 161 --- Val Acc: 77.710 \t Best Acc: 77.710\n",
            "Cycle: 1 Epoch: 181 --- Val Acc: 79.220 \t Best Acc: 79.220\n",
            "Cycle: 1 Epoch: 200 --- Val Acc: 78.810 \t Best Acc: 79.220\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 1/7 || Label set size 5000: Test acc 78.81\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            ">> Train a Model...\n",
            "Cycle: 2 Epoch: 1 --- Val Acc: 75.810 \t Best Acc: 75.810\n",
            "Cycle: 2 Epoch: 21 --- Val Acc: 72.140 \t Best Acc: 75.810\n",
            "Cycle: 2 Epoch: 41 --- Val Acc: 73.810 \t Best Acc: 75.810\n",
            "Cycle: 2 Epoch: 61 --- Val Acc: 74.910 \t Best Acc: 75.810\n",
            "Cycle: 2 Epoch: 81 --- Val Acc: 74.970 \t Best Acc: 75.810\n",
            "Cycle: 2 Epoch: 101 --- Val Acc: 71.260 \t Best Acc: 75.810\n",
            "Cycle: 2 Epoch: 121 --- Val Acc: 72.310 \t Best Acc: 75.810\n",
            "Cycle: 2 Epoch: 141 --- Val Acc: 67.360 \t Best Acc: 75.810\n",
            "Cycle: 2 Epoch: 161 --- Val Acc: 79.740 \t Best Acc: 79.740\n",
            "Cycle: 2 Epoch: 181 --- Val Acc: 81.200 \t Best Acc: 81.200\n",
            "Cycle: 2 Epoch: 200 --- Val Acc: 81.570 \t Best Acc: 81.570\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 2/7 || Label set size 5040: Test acc 81.57\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            ">> Train a Model...\n",
            "Cycle: 3 Epoch: 1 --- Val Acc: 78.860 \t Best Acc: 78.860\n",
            "Cycle: 3 Epoch: 21 --- Val Acc: 72.940 \t Best Acc: 78.860\n",
            "Cycle: 3 Epoch: 41 --- Val Acc: 72.470 \t Best Acc: 78.860\n",
            "Cycle: 3 Epoch: 61 --- Val Acc: 74.130 \t Best Acc: 78.860\n",
            "Cycle: 3 Epoch: 81 --- Val Acc: 72.730 \t Best Acc: 78.860\n",
            "Cycle: 3 Epoch: 101 --- Val Acc: 69.780 \t Best Acc: 78.860\n",
            "Cycle: 3 Epoch: 121 --- Val Acc: 72.400 \t Best Acc: 78.860\n",
            "Cycle: 3 Epoch: 141 --- Val Acc: 70.560 \t Best Acc: 78.860\n",
            "Cycle: 3 Epoch: 161 --- Val Acc: 79.380 \t Best Acc: 79.380\n",
            "Cycle: 3 Epoch: 181 --- Val Acc: 81.310 \t Best Acc: 81.310\n",
            "Cycle: 3 Epoch: 200 --- Val Acc: 81.780 \t Best Acc: 81.780\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 3/7 || Label set size 5072: Test acc 81.78\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            ">> Train a Model...\n",
            "Cycle: 4 Epoch: 1 --- Val Acc: 81.250 \t Best Acc: 81.250\n",
            "Cycle: 4 Epoch: 21 --- Val Acc: 73.120 \t Best Acc: 81.250\n",
            "Cycle: 4 Epoch: 41 --- Val Acc: 73.870 \t Best Acc: 81.250\n",
            "Cycle: 4 Epoch: 61 --- Val Acc: 73.190 \t Best Acc: 81.250\n",
            "Cycle: 4 Epoch: 81 --- Val Acc: 72.930 \t Best Acc: 81.250\n",
            "Cycle: 4 Epoch: 101 --- Val Acc: 72.950 \t Best Acc: 81.250\n",
            "Cycle: 4 Epoch: 121 --- Val Acc: 72.930 \t Best Acc: 81.250\n",
            "Cycle: 4 Epoch: 141 --- Val Acc: 73.340 \t Best Acc: 81.250\n",
            "Cycle: 4 Epoch: 161 --- Val Acc: 79.990 \t Best Acc: 81.250\n",
            "Cycle: 4 Epoch: 181 --- Val Acc: 82.060 \t Best Acc: 82.060\n",
            "Cycle: 4 Epoch: 200 --- Val Acc: 82.280 \t Best Acc: 82.280\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 4/7 || Label set size 5072: Test acc 82.28\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            ">> Train a Model...\n",
            "Cycle: 5 Epoch: 1 --- Val Acc: 81.910 \t Best Acc: 81.910\n",
            "Cycle: 5 Epoch: 21 --- Val Acc: 77.240 \t Best Acc: 81.910\n",
            "Cycle: 5 Epoch: 41 --- Val Acc: 72.880 \t Best Acc: 81.910\n",
            "Cycle: 5 Epoch: 61 --- Val Acc: 68.320 \t Best Acc: 81.910\n",
            "Cycle: 5 Epoch: 81 --- Val Acc: 72.360 \t Best Acc: 81.910\n",
            "Cycle: 5 Epoch: 101 --- Val Acc: 70.910 \t Best Acc: 81.910\n",
            "Cycle: 5 Epoch: 121 --- Val Acc: 74.230 \t Best Acc: 81.910\n",
            "Cycle: 5 Epoch: 141 --- Val Acc: 67.450 \t Best Acc: 81.910\n",
            "Cycle: 5 Epoch: 161 --- Val Acc: 80.530 \t Best Acc: 81.910\n",
            "Cycle: 5 Epoch: 181 --- Val Acc: 82.400 \t Best Acc: 82.400\n",
            "Cycle: 5 Epoch: 200 --- Val Acc: 82.860 \t Best Acc: 82.860\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 5/7 || Label set size 5072: Test acc 82.86\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            ">> Train a Model...\n",
            "Cycle: 6 Epoch: 1 --- Val Acc: 82.680 \t Best Acc: 82.680\n",
            "Cycle: 6 Epoch: 21 --- Val Acc: 67.540 \t Best Acc: 82.680\n",
            "Cycle: 6 Epoch: 41 --- Val Acc: 76.700 \t Best Acc: 82.680\n",
            "Cycle: 6 Epoch: 61 --- Val Acc: 76.610 \t Best Acc: 82.680\n",
            "Cycle: 6 Epoch: 81 --- Val Acc: 75.130 \t Best Acc: 82.680\n",
            "Cycle: 6 Epoch: 101 --- Val Acc: 69.820 \t Best Acc: 82.680\n",
            "Cycle: 6 Epoch: 121 --- Val Acc: 72.090 \t Best Acc: 82.680\n",
            "Cycle: 6 Epoch: 141 --- Val Acc: 75.290 \t Best Acc: 82.680\n",
            "Cycle: 6 Epoch: 161 --- Val Acc: 80.410 \t Best Acc: 82.680\n",
            "Cycle: 6 Epoch: 181 --- Val Acc: 82.090 \t Best Acc: 82.680\n",
            "Cycle: 6 Epoch: 200 --- Val Acc: 82.570 \t Best Acc: 82.680\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 6/7 || Label set size 5072: Test acc 82.57\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            ">> Train a Model...\n",
            "Cycle: 7 Epoch: 1 --- Val Acc: 82.220 \t Best Acc: 82.220\n",
            "Cycle: 7 Epoch: 21 --- Val Acc: 69.240 \t Best Acc: 82.220\n",
            "Cycle: 7 Epoch: 41 --- Val Acc: 77.220 \t Best Acc: 82.220\n",
            "Cycle: 7 Epoch: 61 --- Val Acc: 76.200 \t Best Acc: 82.220\n",
            "Cycle: 7 Epoch: 81 --- Val Acc: 73.570 \t Best Acc: 82.220\n",
            "Cycle: 7 Epoch: 101 --- Val Acc: 72.780 \t Best Acc: 82.220\n",
            "Cycle: 7 Epoch: 121 --- Val Acc: 66.020 \t Best Acc: 82.220\n",
            "Cycle: 7 Epoch: 141 --- Val Acc: 74.800 \t Best Acc: 82.220\n",
            "Cycle: 7 Epoch: 161 --- Val Acc: 81.140 \t Best Acc: 82.220\n",
            "Cycle: 7 Epoch: 181 --- Val Acc: 82.710 \t Best Acc: 82.710\n",
            "Cycle: 7 Epoch: 200 --- Val Acc: 83.020 \t Best Acc: 83.020\n",
            ">> Finished.\n",
            "Trial 1/1 || Cycle 7/7 || Label set size 5072: Test acc 83.02\n",
            ">> Start clustering:\n",
            ">> Clustering Over:\n",
            "---------------------------Current Trial is done-----------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e21adede"
      },
      "source": [
        ""
      ],
      "id": "e21adede",
      "execution_count": 55,
      "outputs": []
    }
  ]
}